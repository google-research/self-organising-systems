{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L8F2pl0H2Szr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "REMOTE_GPU = True\n",
        "# This tutorial doesn't use videos, and therefore doesn't need custom kernels.\n",
        "if REMOTE_GPU:\n",
        "  print(\"here\")\n",
        "\n",
        "  from google3.learning.brain.contrib.eager.python import remote_eager\n",
        "  from google3.third_party.tensorflow.python.eager import context\n",
        "\n",
        "  # Replace with your BNS address (omit the task index)\n",
        "  WORKER_NMB = '14078055'\n",
        "  BNS_ADDRESS = '/bns/li/borg/li/bns/etr/etr_headless_gpu_{}.1.gpu_worker'.format(WORKER_NMB)\n",
        "  NUM_WORKERS = 1\n",
        "\n",
        "  # Connect\n",
        "  print(\"Connecting\")\n",
        "  remote_eager.enable('{}/0'.format(BNS_ADDRESS), num_workers=NUM_WORKERS)\n",
        "  print(\"Connected\")\n",
        "\n",
        "  # Print devices\n",
        "  print(tf.config.experimental.list_logical_devices())\n",
        "\n",
        "#from tensorflow.keras.layers import Dense\n",
        "\n",
        "import matplotlib.pyplot as plt # visualization\n",
        "\n",
        "import numpy as onp\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.compat.v2 as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import IPython.display as display\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3Xbr8zol2fLp"
      },
      "outputs": [],
      "source": [
        "from colabtools import adhoc_import\n",
        "import getpass\n",
        "\n",
        "client_name = 'twp'\n",
        "user_name = getpass.getuser()\n",
        "with adhoc_import.Google3CitcClient(\n",
        "    client_name, user_name) as outputs:\n",
        "  from google3.experimental.selforg.metalearn import core\n",
        "  core = adhoc_import.Reload(core)\n",
        "  from google3.experimental.selforg.metalearn import tf_layers\n",
        "  tf_layers = adhoc_import.Reload(tf_layers)\n",
        "  from google3.experimental.selforg.metalearn import util\n",
        "  util = adhoc_import.Reload(util)\n",
        "  from google3.experimental.selforg.neurca.data import data_processing\n",
        "  data_processing = adhoc_import.Reload(data_processing)\n",
        "  from google3.experimental.selforg.metalearn import training\n",
        "  training = adhoc_import.Reload(training)\n",
        "\n",
        "  #import umap\n",
        "  #from google3.third_party.py.umap import umap_\n",
        "  #umap_ = adhoc_import.Reload(umap_)\n",
        "\n",
        "MPDense = tf_layers.MPDense\n",
        "#MPReshape = layers.MPReshape\n",
        "#MPConv = tf_layers.MPConv\n",
        "#MPWeightNorm = layers.MPWeightNorm\n",
        "MPActivation = tf_layers.MPActivation\n",
        "MPSoftmax = tf_layers.MPSoftmax\n",
        "#MPStandardize = layers.MPStandardize\n",
        "MPCrossEntropyLoss = tf_layers.MPCrossEntropyLoss\n",
        "MPNetwork = tf_layers.MPNetwork\n",
        "MPMetrics = tf_layers.MPMetrics\n",
        "SamplePool = util.SamplePool\n",
        "load_benchmark_MNIST = data_processing.load_benchmark_MNIST\n",
        "TrainingRegime = training.TrainingRegime\n",
        "\n",
        "GRUBlock = core.GRUBlock\n",
        "OutStandardizer = core.OutStandardizer\n",
        "StandardizeInputsAndStates = core.StandardizeInputsAndStates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jKFOzgoO21hv"
      },
      "outputs": [],
      "source": [
        "def one_hottify(dset):\n",
        "  one_hottified = onp.zeros([dset.shape[0], 10], dtype=onp.float32)\n",
        "  one_hottified[onp.arange(dset.size), dset] = 1.0\n",
        "  return one_hottified\n",
        "\n",
        "def MNIST_generator(inputs, labels):\n",
        "  while True:\n",
        "    idx = pyrandom.randrange(len(inputs))\n",
        "    x = inputs[idx]\n",
        "    y = labels[idx]\n",
        "    yield x, y\n",
        "\n",
        "PIC_L = 12\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_benchmark_MNIST()\n",
        "x_train = tf.image.resize(onp.expand_dims(x_train, -1), size=(PIC_L, PIC_L)).numpy()\n",
        "x_test = tf.image.resize(onp.expand_dims(x_test, -1), size=(PIC_L, PIC_L)).numpy()\n",
        "y_train = one_hottify(y_train)\n",
        "y_test = one_hottify(y_test)\n",
        "\n",
        "# standardize inputs:\n",
        "train_mean, train_std = x_train.mean(), x_train.std()\n",
        "\n",
        "x_train = (x_train - train_mean) / train_std\n",
        "x_test = (x_test - train_mean) / train_std\n",
        "\n",
        "x_train = x_train.reshape([-1, PIC_L * PIC_L])\n",
        "x_test = x_test.reshape([-1, PIC_L * PIC_L])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lzbQbrDy3UW1"
      },
      "outputs": [],
      "source": [
        "outer_batch_size = 1\n",
        "inner_batch_size = 8\n",
        "loop_steps = 20\n",
        "img_h = img_w = 12\n",
        "\n",
        "def resize(x, y):\n",
        "  x = tf.image.resize(x, [img_h, img_w])\n",
        "  x = tf.reshape(x, [-1, img_h * img_w])\n",
        "  return x, y\n",
        "\n",
        "resized_mnist_train = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train))#.map(resize)\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "  dataset = resized_mnist_train.batch(inner_batch_size).batch(loop_steps).batch(outer_batch_size).shuffle(1000).cache().repeat()\n",
        "  ds_iter = iter(dataset)\n",
        "  xval_ds = resized_mnist_train.batch(inner_batch_size).batch(outer_batch_size).shuffle(1000).cache().repeat()\n",
        "  xval_ds_iter = iter(xval_ds)\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_generator(\n",
        "            lambda: MNIST_generator(x_test, y_test),\n",
        "            output_types=(onp.float32, onp.float32))\n",
        "  test_ds = test_ds.batch(inner_batch_size)\n",
        "  test_ds_iter = iter(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "B4n7iH6e2lYz"
      },
      "outputs": [],
      "source": [
        "message_size = 4\n",
        "stateful = True\n",
        "stateful_hidden_n = 7#15\n",
        "\n",
        "MPMetrics.reset()\n",
        "import functools\n",
        "\n",
        "l0 = PIC_L * PIC_L\n",
        "def init_network(activation, sizes):\n",
        "  network = MPNetwork([MPDense(sizes[1], stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPActivation(activation, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPDense(10, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPSoftmax(stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    ],\n",
        "                    MPCrossEntropyLoss(message_size, stateful=stateful, stateful_hidden_n=stateful_hidden_n))\n",
        "  # create shared networks!\n",
        "  shared_params = {\n",
        "      \"W_net\": GRUBlock(x_dim=2 + message_size,\n",
        "                        carry_n=1+message_size+stateful_hidden_n),\n",
        "      \"b_net\": GRUBlock(x_dim=2 + message_size,\n",
        "                        carry_n=1+message_size+stateful_hidden_n),\n",
        "#      \"W_out_std\": OutStandardizer(scale_init_val=0.05),\n",
        "#      \"b_out_std\": OutStandardizer(scale_init_val=0.05),\n",
        "#      \"W_in_std\": StandardizeInputsAndStates([\"W_b\", \"W_in\"]),\n",
        "#      \"b_in_std\": StandardizeInputsAndStates([\"b_b\", \"b_in\"]),\n",
        "                   }\n",
        "\n",
        "  network.setup(in_dim=sizes[0], message_size=message_size, \n",
        "                inner_batch_size=inner_batch_size,\n",
        "                shared_params=shared_params)\n",
        "  return network\n",
        "\n",
        "network = init_network(tf.sigmoid, (l0, 50))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "y1m8X8pMJnfv"
      },
      "outputs": [],
      "source": [
        "# load from drive\n",
        "from colabtools import drive\n",
        "#from google.colab import drive\n",
        "drive.ListFolder(\"16EFaQPhjfj4oYEYzJ_e6DCqmfP_mVVSW\")#/My%20Drive\")#/selforg/Learn\\ 2\\ Learn/mnist\")\n",
        "\n",
        "weights_byte = drive.LoadFile(file_id=\"13sU-0KGLPFHsDZS2mOB1o7tF-yHIOF9t\")\n",
        "print(weights_byte)\n",
        "!ls -lh tmp\n",
        "with open(\"tmp/weights_00005000.npy\", \"wb\") as fout:\n",
        "  fout.write(weights_byte)\n",
        "  #arr = np.fromstring(weights_byte)\n",
        "  #np.save(fout, arr)\n",
        "!ls -lh tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FwUQGYlN4V19"
      },
      "outputs": [],
      "source": [
        "!ls -lh tmp\n",
        "network.load_weights(\"tmp/weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IKe2E8miWnpM"
      },
      "outputs": [],
      "source": [
        "def create_new_p_fw():\n",
        "  return network.init(inner_batch_size)\n",
        "\n",
        "POOL_SIZE = 16\n",
        "\n",
        "pool = SamplePool(ps=tf.stack(\n",
        "    [network.serialize_pfw(create_new_p_fw()) for _ in range(POOL_SIZE)]).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jtVVwKNrWzpt"
      },
      "outputs": [],
      "source": [
        "loop_steps_t = tf.constant(loop_steps)\n",
        "\n",
        "learning_schedule = 5e-4\n",
        "\n",
        "# Prepare a training regime.\n",
        "# The heldout_weight tells you how to split the loss between train and eval sets\n",
        "# that are passed to the network.\n",
        "# Empirically, a heldout_weight=0.0 (or None), results in a much lower overall\n",
        "# performance, both for train and test losses.\n",
        "training_regime = TrainingRegime(\n",
        "    network, heldout_weight=1.0, hint_loss_ratio=0.7, remember_loss_ratio=None)\n",
        "\n",
        "last_step = 0\n",
        "\n",
        "# Initialize important parameters by passing a minibatch.\n",
        "#x_b, y_b, _, _ = next(ds_iter)\n",
        "#network.minibatch_init(x_b[0][0],  y_b[0][0], INNER_BATCH_SIZE)\n",
        "\n",
        "# enhanced version of minibatch init, allowing to initialize by looking at more\n",
        "# than just one step.\n",
        "# Likewise, this can be run multiple times to improve the initialization.\n",
        "for j in range(5):\n",
        "  print(\"on\", j)\n",
        "  stats = []\n",
        "  pfw = network.init(inner_batch_size)\n",
        "\n",
        "  x_b, y_b = next(ds_iter)\n",
        "  x_b, y_b = x_b[0], y_b[0]\n",
        "  for i in range(loop_steps):\n",
        "    # Initialize important parameters by passing a minibatch.\n",
        "    #x_b, y_b = x_eval, target_eval # a minibatch, basically. Could be prettified.\n",
        "\n",
        "    pfw, stats_i = network.minibatch_init(x_b[i],  y_b[i], x_b[i].shape[0], pfw=pfw)\n",
        "    stats.append(stats_i)\n",
        "  # update\n",
        "  network.update_statistics(stats, update_perc=0.5)\n",
        "\n",
        "  print(\"final mean:\")\n",
        "  for p in tf.nest.flatten(pfw):\n",
        "    print(p.shape, tf.reduce_mean(p), tf.math.reduce_std(p))\n",
        "\n",
        "\n",
        "\n",
        "# The outer loop here uses Adam. SGD/Momentum are more stable but way slower.\n",
        "#trainer = tf.keras.optimizers.Adam(learning_schedule)\n",
        "trainer = tf.keras.optimizers.Adam(learning_schedule)\n",
        "\n",
        "loss_log = []\n",
        "def smoothen(l, lookback=20):\n",
        "  # first of all, if it's a nan, change it to a high value\n",
        "  kernel = [1./lookback] * lookback\n",
        "  return np.convolve(l[0:1] * (lookback - 1) + l, kernel, \"valid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4cByI6hXGbHD"
      },
      "outputs": [],
      "source": [
        "!mkdir tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AilvJiIj3zhS"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_steps = 100000\n",
        "\n",
        "#steps_scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay([500, 1000, 1500, 2000, 5000], [2, 10, 15, 20, 30, 50])\n",
        "#steps_scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay([5000], [50, 50])\n",
        "\n",
        "learning_schedule = 1e-4#optimizers.piecewise_constant([1000, 2000], [1e-5, 1e-4, 1e-3])\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def step(pfws, xts, yts, xes, yes, num_steps):\n",
        "  print(\"compiling\")\n",
        "  with tf.GradientTape() as g:\n",
        "    #Â As you can see, we pass train and eval/heldout instances.\n",
        "    # We only show the network the xts and yts for the inner update,\n",
        "    # but we compute a cross-validation loss using also xes and yes.\n",
        "    l, _, _ = training_regime.batch_mp_loss(pfws, xts, yts, xes, yes, num_steps)\n",
        "  all_weights = network.get_trainable_weights()\n",
        "  grads = g.gradient(l, all_weights)\n",
        "  # Try grad clipping to avoid explosions.\n",
        "  grads = [g/(tf.norm(g)+1e-8) for g in grads]\n",
        "  trainer.apply_gradients(zip(grads, all_weights))\n",
        "  return l\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#MPMetrics.reset_metrics()\n",
        "\n",
        "for i in range(last_step + 1, last_step +1 + training_steps):\n",
        "  last_step = i\n",
        "\n",
        "  #num_loops = loop_steps # steps_scheduler(last_step)\n",
        "\n",
        "  tmp_t = time.time()\n",
        "  xts, yts = next(ds_iter)\n",
        "  xes, yes = next(xval_ds_iter)\n",
        "\n",
        "  batch = pool.sample(outer_batch_size)\n",
        "  fwps = batch.ps\n",
        "\n",
        "  l = step(fwps, xts, yts, xes, yes, loop_steps_t)\n",
        "  loss_log.append(l)\n",
        "  # Metrics aggregation is slow. Consider doing once in a while to now interrupt\n",
        "  #  all the time\n",
        "  #add_metric_episode(metrics_series, merge_metric_batch(metrics))\n",
        "\n",
        "\n",
        "  if i % 50 == 0:\n",
        "    print(i)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  if i % 500 == 0:\n",
        "    plt.plot(smoothen(loss_log, 100), label='mp')\n",
        "    plt.yscale('log')\n",
        "    #plt.ylim(0.0, 1e-1)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  if i % 2500 == 0:\n",
        "    file_path = \"tmp/weights\"\n",
        "    network.save_weights(file_path, last_step)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "np_batched_stl_mpv2_nograd_loss = time_series\n",
        "\n",
        "\n",
        "\n",
        "#print(\"After\")\n",
        "#print_dicts(p_fw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0QWqVlQ5_uiK"
      },
      "outputs": [],
      "source": [
        "!ls tmp -lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "evt-q7nU8eC4"
      },
      "outputs": [],
      "source": [
        "def smoothen(l, lookback=20):\n",
        "  kernel = [1./lookback] * lookback\n",
        "  return onp.convolve(l[0:1] * (lookback - 1) + l, kernel, \"valid\")\n",
        "\n",
        "plt.plot(smoothen(loss_log, 100), label='mp')\n",
        "plt.yscale('log')\n",
        "plt.ylim(1e-2, 3e-1)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lNR0lsrVRoKn"
      },
      "outputs": [],
      "source": [
        "# save the last model\n",
        "! ls tmp -lh\n",
        "%download_file tmp/weights_00005000.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aE5CYsy-ad4h"
      },
      "outputs": [],
      "source": [
        "# @title Optionally, load from drive.\n",
        "# load the proper file.\n",
        "loaded_weights = %upload_files\n",
        "print(loaded_weights)\n",
        "!mkdir tmp\n",
        "f = open('tmp/weights_00005000.npy', 'wb')\n",
        "f.write(loaded_weights['weights_00005000.npy'])\n",
        "f.close()\n",
        "a = np.load('tmp/weights_00005000.npy')\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MhNHDPzjfwYt"
      },
      "outputs": [],
      "source": [
        "#@title if you loaded the file, reload the network\n",
        "\n",
        "network = init_network(tf.sigmoid, (l0, 50))\n",
        "network.load_weights(\"tmp/weights\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "D-Cuifh7wtuo"
      },
      "outputs": [],
      "source": [
        "# Now let's generate a training regime of 10 steps, and compare with SGD.\n",
        "\n",
        "eval_pfw = create_new_p_fw()\n",
        "\n",
        "def prepare_for_analysis(pfw):\n",
        "  return tf.concat([tf.reshape(t, [-1]) for t in pfw], 0).numpy()\n",
        "\n",
        "\n",
        "print(prepare_for_analysis(eval_pfw).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MZTnYwEtQ1Sq"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sp_bs = 100\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "  dataset_sp = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  dataset_sp = dataset_sp.batch(inner_batch_size).shuffle(1000).repeat()\n",
        "  dataset_sp_iter = iter(dataset_sp)\n",
        "  test_sp_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "  test_sp_ds = test_sp_ds.batch(test_sp_bs).repeat()\n",
        "  test_sp_ds_iter = iter(test_sp_ds)\n",
        "\n",
        "\n",
        "def get_accuracy(pfw, xe, ye):\n",
        "  targets = tf.argmax(ye, axis=-1)\n",
        "  res, _, _ = network.forward(pfw, xe)\n",
        "  predictions = tf.argmax(res, axis=-1)\n",
        "\n",
        "  tot_correct = tf.reduce_sum(tf.cast(tf.equal(predictions, targets), tf.float32))\n",
        "  accuracy = tot_correct / ye.shape[0]\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vPYIYhBw93AZ"
      },
      "outputs": [],
      "source": [
        "# MP step accuracy\n",
        "all_MP_series = []\n",
        "eval_tot_steps = 100\n",
        "ev_losses = np.zeros([eval_tot_steps, loop_steps])\n",
        "for s in range(eval_tot_steps):\n",
        "  if s % 10 == 0:\n",
        "    print(\"\\nRepetition {}\".format(s))\n",
        "  MP_params_series = []\n",
        "\n",
        "  mp_pfw = network.init(inner_batch_size)\n",
        "\n",
        "  #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "  #MP_params_series.append(mp_pfw)\n",
        "\n",
        "  for i in range(loop_steps):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_iter)\n",
        "    \n",
        "    mp_pfw, _, _ = network.inner_update(mp_pfw, xt, yt)\n",
        "    #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "    #MP_params_series.append(mp_pfw)\n",
        "\n",
        "    xe, ye = next(test_sp_ds_iter)\n",
        "    accuracy = get_accuracy(mp_pfw, xe, ye)\n",
        "    ev_losses[s, i] = accuracy\n",
        "\n",
        "    #print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "ev_losses_m = np.mean(ev_losses, axis=0)\n",
        "ev_losses_sd = np.std(ev_losses, axis=0)\n",
        "print(\"mean:\", ev_losses_m)\n",
        "print(\"sd:\", ev_losses_sd)\n",
        "\n",
        "ub = [m + sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "lb = [m - sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "plt.fill_between(range(1, len(ev_losses_m) + 1), ub, lb, alpha=.5)\n",
        "plt.plot(range(1, len(ev_losses_m) + 1), ev_losses_m, label='eval loss')\n",
        "#plt.ylim(0.0, 0.04)\n",
        "plt.xlabel(\"num steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "  #all_MP_series.append(MP_params_series)\n",
        "\n",
        "mp_baseline_m = ev_losses_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UFJaDaXU8ySM"
      },
      "outputs": [],
      "source": [
        "# MP step\n",
        "all_MP_series = []\n",
        "for s in range(10):\n",
        "  print(\"\\nRepetition {}\".format(s))\n",
        "  MP_params_series = []\n",
        "\n",
        "  mp_pfw = network.init(inner_batch_size)\n",
        "\n",
        "  #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "  #MP_params_series.append(mp_pfw)\n",
        "\n",
        "  for i in range(10):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_iter)\n",
        "    \n",
        "    mp_pfw, _, _ = inner_update(mp_pfw, xt, yt)\n",
        "    #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "    #MP_params_series.append(mp_pfw)\n",
        "\n",
        "    xe, ye = next(test_sp_ds_iter)\n",
        "    accuracy = get_accuracy(mp_pfw, xe, ye)\n",
        "    print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "\n",
        "\n",
        "  #all_MP_series.append(MP_params_series)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PGlLBdbhBPmu"
      },
      "outputs": [],
      "source": [
        "# ADAM network\n",
        "adam_network = MPNetwork([MPDense(50, stateful=False),\n",
        "                  MPActivation(tf.sigmoid, stateful=False),\n",
        "                  MPDense(10, stateful=False),\n",
        "                  MPSoftmax(stateful=False),\n",
        "                  ],\n",
        "                  MPCrossEntropyLoss(message_size, stateful=False))\n",
        "adam_network.setup(in_dim=l0, message_size=message_size)\n",
        "\n",
        "\n",
        "\n",
        "def get_adam_accuracy(pfw, xe, ye):\n",
        "  targets = tf.argmax(ye, axis=-1)\n",
        "  res, _, _ = adam_network.forward(pfw, xe)\n",
        "  predictions = tf.argmax(res, axis=-1)\n",
        "\n",
        "  tot_correct = tf.reduce_sum(tf.cast(tf.equal(predictions, targets), tf.float32))\n",
        "  accuracy = tot_correct / ye.shape[0]\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OC8Zszt88fIx"
      },
      "outputs": [],
      "source": [
        "# SGD step\n",
        "\n",
        "\n",
        "\n",
        "all_SGD_series = []\n",
        "eval_tot_steps = 100\n",
        "sgd_ev_losses = np.zeros([eval_tot_steps, loop_steps])\n",
        "for s in range(eval_tot_steps):\n",
        "  if s % 10 == 0:\n",
        "    print(\"\\nRepetition {}\".format(s))\n",
        "  #print(\"on\", s)\n",
        "  SGD_params_series = []\n",
        "\n",
        "  sgd_pfw = [tf.Variable(t) for t in adam_network.init()]\n",
        "\n",
        "  #SGD_params_series.append(prepare_for_analysis(sgd_pfw))\n",
        "  #SGD_params_series.append(sgd_pfw)\n",
        "\n",
        "  adam_trainer = tf.keras.optimizers.SGD(0.1)\n",
        "\n",
        "  @tf.function\n",
        "  def step(xt, yt):\n",
        "    with tf.GradientTape() as g:\n",
        "      g.watch(sgd_pfw)\n",
        "      y, _, _ = adam_network.forward(sgd_pfw, xt)\n",
        "      l, _ = adam_network.compute_loss(y, yt)\n",
        "      l = tf.reduce_mean(tf.reduce_sum(l, axis=[1]))\n",
        "\n",
        "    grads = g.gradient(l, sgd_pfw)\n",
        "    adam_trainer.apply_gradients(zip(grads, sgd_pfw))\n",
        "\n",
        "\n",
        "  for i in range(loop_steps):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_iter)\n",
        "    step(xt, yt)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      xe, ye = next(test_sp_ds_iter)\n",
        "      accuracy = get_adam_accuracy(sgd_pfw, xe, ye)\n",
        "      sgd_ev_losses[s, i] = accuracy\n",
        "      #print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "      #SGD_params_series.append(sgd_pfw)\n",
        "      #SGD_params_series.append(prepare_for_analysis(sgd_pfw))\n",
        "  #all_SGD_series.append(SGD_params_series)\n",
        "\n",
        "ev_losses_m = np.mean(sgd_ev_losses, axis=0)\n",
        "ev_losses_sd = np.std(sgd_ev_losses, axis=0)\n",
        "print(\"mean:\", ev_losses_m)\n",
        "print(\"sd:\", ev_losses_sd)\n",
        "\n",
        "ub = [m + sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "lb = [m - sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "plt.fill_between(range(1, len(ev_losses_m) + 1), ub, lb, alpha=.5)\n",
        "plt.plot(range(1, len(ev_losses_m) + 1), ev_losses_m, label='eval loss')\n",
        "#plt.ylim(0.0, 0.04)\n",
        "plt.xlabel(\"num steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "sgd_m = ev_losses_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wOWy2FaUGARI"
      },
      "outputs": [],
      "source": [
        "# Adam step\n",
        "\n",
        "\n",
        "\n",
        "all_SGD_series = []\n",
        "eval_tot_steps = 100\n",
        "sgd_ev_losses = np.zeros([eval_tot_steps, loop_steps])\n",
        "for s in range(eval_tot_steps):\n",
        "  if s % 10 == 0:\n",
        "    print(\"\\nRepetition {}\".format(s))\n",
        "  #print(\"on\", s)\n",
        "  SGD_params_series = []\n",
        "\n",
        "  sgd_pfw = [tf.Variable(t) for t in adam_network.init()]\n",
        "\n",
        "  #SGD_params_series.append(prepare_for_analysis(sgd_pfw))\n",
        "  #SGD_params_series.append(sgd_pfw)\n",
        "\n",
        "  adam_trainer = tf.keras.optimizers.Adam(0.01)\n",
        "\n",
        "  @tf.function\n",
        "  def step(xt, yt):\n",
        "    with tf.GradientTape() as g:\n",
        "      g.watch(sgd_pfw)\n",
        "      y, _, _ = adam_network.forward(sgd_pfw, xt)\n",
        "      l, _ = adam_network.compute_loss(y, yt)\n",
        "      l = tf.reduce_mean(tf.reduce_sum(l, axis=[1]))\n",
        "\n",
        "    grads = g.gradient(l, sgd_pfw)\n",
        "    adam_trainer.apply_gradients(zip(grads, sgd_pfw))\n",
        "\n",
        "\n",
        "  for i in range(loop_steps):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_iter)\n",
        "    step(xt, yt)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      xe, ye = next(test_sp_ds_iter)\n",
        "      accuracy = get_adam_accuracy(sgd_pfw, xe, ye)\n",
        "      sgd_ev_losses[s, i] = accuracy\n",
        "      #print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "      #SGD_params_series.append(sgd_pfw)\n",
        "      #SGD_params_series.append(prepare_for_analysis(sgd_pfw))\n",
        "  #all_SGD_series.append(SGD_params_series)\n",
        "\n",
        "ev_losses_m = np.mean(sgd_ev_losses, axis=0)\n",
        "ev_losses_sd = np.std(sgd_ev_losses, axis=0)\n",
        "print(\"mean:\", ev_losses_m)\n",
        "print(\"sd:\", ev_losses_sd)\n",
        "\n",
        "ub = [m + sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "lb = [m - sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "plt.fill_between(range(1, len(ev_losses_m) + 1), ub, lb, alpha=.5)\n",
        "plt.plot(range(1, len(ev_losses_m) + 1), ev_losses_m, label='eval loss')\n",
        "#plt.ylim(0.0, 0.04)\n",
        "plt.xlabel(\"num steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "adam_m = ev_losses_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PxPKxkvHGfDV"
      },
      "outputs": [],
      "source": [
        "raise Exception(\"Don't run code below! it'd destroy your model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "K6BeqhmvC_iX"
      },
      "outputs": [],
      "source": [
        "# ADAM step\n",
        "\n",
        "all_ADAM_series = []\n",
        "for s in range(10):\n",
        "  print(\"on\", s)\n",
        "  ADAM_params_series = []\n",
        "\n",
        "  adam_pfw = [tf.Variable(t) for t in eval_pfw]\n",
        "\n",
        "  ADAM_params_series.append(prepare_for_analysis(adam_pfw))\n",
        "  #ADAM_params_series.append(adam_pfw)\n",
        "\n",
        "  trainer = tf.keras.optimizers.Adam(0.01)\n",
        "\n",
        "\n",
        "  for i in range(10):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(test_ds_iter)\n",
        "    targets = tf.argmax(yt, axis=-1)\n",
        "\n",
        "    with tf.GradientTape() as g:\n",
        "      g.watch(adam_pfw)\n",
        "      y, _, _ = network.forward(adam_pfw, xt)\n",
        "      l, _ = network.compute_loss(y, yt)\n",
        "      l = tf.reduce_mean(tf.reduce_sum(l, axis=[1]))\n",
        "\n",
        "    grads = g.gradient(l, adam_pfw)\n",
        "    trainer.apply_gradients(zip(grads, adam_pfw))\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      accuracy = get_accuracy(adam_pfw, xe, ye)\n",
        "      print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "      ADAM_params_series.append(prepare_for_analysis(adam_pfw))\n",
        "      #ADAM_params_series.append(adam_pfw)\n",
        "\n",
        "  all_ADAM_series.append(ADAM_params_series)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "179ijpbny6Gx"
      },
      "outputs": [],
      "source": [
        "# MP step but starting from a new pfw\n",
        "\n",
        "new_eval_pfw = create_new_p_fw()\n",
        "\n",
        "all_MP_series_v2 = []\n",
        "for s in range(10):\n",
        "  print(\"\\nRepetition {}\".format(s))\n",
        "  MP_params_series = []\n",
        "\n",
        "  mp_pfw = new_eval_pfw\n",
        "\n",
        "  MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "  #MP_params_series.append(mp_pfw)\n",
        "\n",
        "  for i in range(10):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_iter)\n",
        "    \n",
        "    mp_pfw, _, _ = inner_update(mp_pfw, xt, yt)\n",
        "    MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "    #MP_params_series.append(mp_pfw)\n",
        "\n",
        "    xe, ye = next(test_sp_ds_iter)\n",
        "    accuracy = get_accuracy(mp_pfw, xe, ye)\n",
        "    print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "\n",
        "\n",
        "  all_MP_series_v2.append(MP_params_series)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wMB55zAp0I3n"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Dy7vsf4T0OE3"
      },
      "outputs": [],
      "source": [
        "all_inputs = []\n",
        "labels = []\n",
        "\n",
        "add_iter = False\n",
        "\n",
        "for idx, s in enumerate(all_MP_series):\n",
        "\n",
        "  all_inputs += s\n",
        "  curr_lbl = \"MP\"\n",
        "  curr_lbl += \"_{}\".format(idx) if add_iter else \"\"\n",
        "  labels += [curr_lbl for _ in range(len(s))]\n",
        "\n",
        "for idx, s in enumerate(all_SGD_series):\n",
        "  all_inputs += s\n",
        "  curr_lbl = \"SGD_1e-1\"\n",
        "  curr_lbl += \"_{}\".format(idx) if add_iter else \"\"  \n",
        "  labels += [curr_lbl for _ in range(len(s))]\n",
        "\n",
        "for idx, s in enumerate(all_ADAM_series):\n",
        "  all_inputs += s\n",
        "  curr_lbl = \"ADAM_1e-2\"\n",
        "  curr_lbl += \"_{}\".format(idx) if add_iter else \"\"  \n",
        "  labels += [curr_lbl for _ in range(len(s))]\n",
        "\n",
        "for idx, s in enumerate(all_MP_series_v2):\n",
        "  all_inputs += s\n",
        "  curr_lbl = \"MP_v2\"\n",
        "  curr_lbl += \"_{}\".format(idx) if add_iter else \"\"\n",
        "  labels += [curr_lbl for _ in range(len(s))]\n",
        "\n",
        "X = np.stack(all_inputs)\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EMvTjkT51_ha"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8pbjypI9525W"
      },
      "outputs": [],
      "source": [
        "# [\"MP_{}\".format(i) for i in range(X.shape[0])]\n",
        "df = pd.DataFrame({\"label\": labels,\n",
        "                   \"pca-one\": pca_result[:, 0], \"pca-two\": pca_result[:,1]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rCjuZaH-5_02"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(16,10))\n",
        "\n",
        "sns.scatterplot(\n",
        "    x=\"pca-one\", y=\"pca-two\",\n",
        "    hue=\"label\",\n",
        "    #palette=sns.color_palette(\"hls\", 11),\n",
        "    data=df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "#sns.lineplot(x=\"pca-one\", y=\"pca-two\", hue=\"label\", data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2ZT_5iXfFfXW"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "MP_inst = all_MP_series[0]\n",
        "MP_inst_v2 = all_MP_series_v2[0]\n",
        "mp_origin = MP_inst[0]\n",
        "sgd_inst = all_SGD_series[0]\n",
        "adam_inst = all_ADAM_series[0]\n",
        "for i in range(1, len(MP_inst)):\n",
        "  mp_next = MP_inst[i] - mp_origin\n",
        "  mp_v2_next = MP_inst_v2[i] - mp_origin\n",
        "  sgd_next = sgd_inst[i] - mp_origin\n",
        "  adam_next = adam_inst[i] - mp_origin\n",
        "  print(scipy.spatial.distance.cosine(mp_origin, mp_next),\n",
        "        scipy.spatial.distance.cosine(mp_origin, sgd_next),\n",
        "        scipy.spatial.distance.cosine(mp_origin, adam_next),\n",
        "        scipy.spatial.distance.cosine(mp_next, sgd_next),\n",
        "        scipy.spatial.distance.cosine(mp_next, adam_next),\n",
        "        scipy.spatial.distance.cosine(sgd_next, adam_next),\n",
        "        scipy.spatial.distance.cosine(mp_next, mp_v2_next))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "09WKw-CIZFmR"
      },
      "source": [
        "# See what happens if you change activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oRBNnC3GZQSC"
      },
      "outputs": [],
      "source": [
        "#message_size = 4#8\n",
        "#stateful = True\n",
        "#stateful_hidden_n = 7#15\n",
        "\n",
        "\n",
        "new_activation = lambda x: tf.maximum(0.0, tf.sign(x))\n",
        "\n",
        "l0 = PIC_L * PIC_L\n",
        "\"\"\"\n",
        "def init_new_network():\n",
        "  new_network = MPNetwork([MPDense(50, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPActivation(new_activation, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPDense(10, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPSoftmax(stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    ],\n",
        "                    MPCrossEntropyLoss(message_size, stateful=stateful, stateful_hidden_n=stateful_hidden_n))\n",
        "  new_network.setup(in_dim=l0, message_size=message_size, inner_batch_size=inner_batch_size)\n",
        "  return new_network\"\"\"\n",
        "\n",
        "new_network = init_network(new_activation, (l0, 50))\n",
        "new_network.load_weights(\"tmp/weights\")\n",
        "\n",
        "def get_accuracy(net, pfw, xe, ye):\n",
        "  targets = tf.argmax(ye, axis=-1)\n",
        "  res, _, _ = net.forward(pfw, xe)\n",
        "  predictions = tf.argmax(res, axis=-1)\n",
        "\n",
        "  tot_correct = tf.reduce_sum(tf.cast(tf.equal(predictions, targets), tf.float32))\n",
        "  accuracy = tot_correct / ye.shape[0]\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X8crDt9UZfzU"
      },
      "outputs": [],
      "source": [
        "# MP step accuracy\n",
        "all_MP_series = []\n",
        "eval_tot_steps = 100\n",
        "ev_losses = np.zeros([eval_tot_steps, loop_steps])\n",
        "for s in range(eval_tot_steps):\n",
        "  if s % 10 == 0:\n",
        "    print(\"\\nRepetition {}\".format(s))\n",
        "  MP_params_series = []\n",
        "\n",
        "  mp_pfw = new_network.init(inner_batch_size)\n",
        "\n",
        "  #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "  #MP_params_series.append(mp_pfw)\n",
        "\n",
        "  for i in range(loop_steps):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_iter)\n",
        "    \n",
        "    mp_pfw, _, _ = new_network.inner_update(mp_pfw, xt, yt)\n",
        "    #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "    #MP_params_series.append(mp_pfw)\n",
        "\n",
        "    xe, ye = next(test_sp_ds_iter)\n",
        "    accuracy = get_accuracy(new_network, mp_pfw, xe, ye)\n",
        "    ev_losses[s, i] = accuracy\n",
        "\n",
        "    #print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "ev_losses_m = np.mean(ev_losses, axis=0)\n",
        "ev_losses_sd = np.std(ev_losses, axis=0)\n",
        "print(\"mean:\", ev_losses_m)\n",
        "print(\"sd:\", ev_losses_sd)\n",
        "\n",
        "ub = [m + sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "lb = [m - sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "plt.fill_between(range(1, len(ev_losses_m) + 1), ub, lb, alpha=.5)\n",
        "plt.plot(range(1, len(ev_losses_m) + 1), ev_losses_m, label='eval loss')\n",
        "#plt.ylim(0.0, 0.04)\n",
        "plt.xlabel(\"num steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "  #all_MP_series.append(MP_params_series)\n",
        "mp_stepf_m = ev_losses_m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4IsIapATTYv"
      },
      "source": [
        "# See what happens if you transfer the learned parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RWnqnkoSTjCH"
      },
      "outputs": [],
      "source": [
        "# big version of MNIST\n",
        "(x_train_b, y_train_b), (x_test_b, y_test_b) = load_benchmark_MNIST()\n",
        "y_train_b = one_hottify(y_train_b)\n",
        "y_test_b = one_hottify(y_test_b)\n",
        "\n",
        "# standardize inputs:\n",
        "train_mean, train_std = x_train_b.mean(), x_train_b.std()\n",
        "\n",
        "x_train_b = (x_train_b - train_mean) / train_std\n",
        "x_test_b = (x_test_b - train_mean) / train_std\n",
        "\n",
        "x_train_b = x_train_b.reshape([-1, 28 * 28])\n",
        "x_test_b = x_test_b.reshape([-1, 28* 28])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "166Gjk3OTZ5w"
      },
      "outputs": [],
      "source": [
        "#message_size = 4#8\n",
        "#stateful = True\n",
        "#stateful_hidden_n = 7#15\n",
        "\n",
        "MPMetrics.reset()\n",
        "import functools\n",
        "\"\"\"\n",
        "def init_big_network():\n",
        "  new_network = MPNetwork([MPDense(100, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPActivation(tf.sigmoid, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPDense(10, stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    MPSoftmax(stateful=stateful, stateful_hidden_n=stateful_hidden_n),\n",
        "                    ],\n",
        "                    MPCrossEntropyLoss(message_size, stateful=stateful, stateful_hidden_n=stateful_hidden_n))\n",
        "  new_network.setup(in_dim=28 * 28, message_size=message_size, inner_batch_size=inner_batch_size)\n",
        "  return new_network\"\"\"\n",
        "\n",
        "new_network = init_network(tf.sigmoid, (28*28, 100))\n",
        "new_network.load_weights(\"tmp/weights\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1beVZtck3TOh"
      },
      "outputs": [],
      "source": [
        "!ls tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4NbxAZDaUr3c"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_sp_bs = 100\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "  dataset_sp_b = tf.data.Dataset.from_tensor_slices((x_train_b, y_train_b))\n",
        "  dataset_sp_b = dataset_sp_b.batch(inner_batch_size).shuffle(1000).repeat()\n",
        "  dataset_sp_b_iter = iter(dataset_sp_b)\n",
        "  test_sp_b_ds = tf.data.Dataset.from_tensor_slices((x_test_b, y_test_b))\n",
        "  test_sp_b_ds = test_sp_b_ds.batch(test_sp_bs).repeat()\n",
        "  test_sp_b_ds_iter = iter(test_sp_b_ds)\n",
        "\n",
        "\n",
        "def get_accuracy(pfw, xe, ye):\n",
        "  targets = tf.argmax(ye, axis=-1)\n",
        "  res, _, _ = new_network.forward(pfw, xe)\n",
        "  predictions = tf.argmax(res, axis=-1)\n",
        "\n",
        "  tot_correct = tf.reduce_sum(tf.cast(tf.equal(predictions, targets), tf.float32))\n",
        "  accuracy = tot_correct / ye.shape[0]\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xgw22TAvUr3g"
      },
      "outputs": [],
      "source": [
        "# MP step accuracy\n",
        "all_MP_series = []\n",
        "eval_tot_steps = 100\n",
        "ev_losses = np.zeros([eval_tot_steps, loop_steps])\n",
        "for s in range(eval_tot_steps):\n",
        "  if s % 10 == 0:\n",
        "    print(\"\\nRepetition {}\".format(s))\n",
        "  MP_params_series = []\n",
        "\n",
        "  mp_pfw = new_network.init(inner_batch_size)\n",
        "\n",
        "  #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "  #MP_params_series.append(mp_pfw)\n",
        "\n",
        "  for i in range(loop_steps):\n",
        "    # using test to train too, because we want to see the effect of learning\n",
        "    # on the parameter space.\n",
        "    xt, yt = next(dataset_sp_b_iter)\n",
        "    \n",
        "    mp_pfw, _, _ = new_network.inner_update(mp_pfw, xt, yt)\n",
        "    #MP_params_series.append(prepare_for_analysis(mp_pfw))\n",
        "    #MP_params_series.append(mp_pfw)\n",
        "\n",
        "    xe, ye = next(test_sp_b_ds_iter)\n",
        "    accuracy = get_accuracy(mp_pfw, xe, ye)\n",
        "    ev_losses[s, i] = accuracy\n",
        "\n",
        "    #print(\"step {}: accuracy {}\". format(i, float(accuracy)))\n",
        "ev_losses_m = np.mean(ev_losses, axis=0)\n",
        "ev_losses_sd = np.std(ev_losses, axis=0)\n",
        "print(\"mean:\", ev_losses_m)\n",
        "print(\"sd:\", ev_losses_sd)\n",
        "\n",
        "ub = [m + sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "lb = [m - sd for m, sd in zip(ev_losses_m, ev_losses_sd)]\n",
        "plt.fill_between(range(1, len(ev_losses_m) + 1), ub, lb, alpha=.5)\n",
        "plt.plot(range(1, len(ev_losses_m) + 1), ev_losses_m, label='eval loss')\n",
        "#plt.ylim(0.0, 0.04)\n",
        "plt.xlabel(\"num steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "  #all_MP_series.append(MP_params_series)\n",
        "mp_big_net_m = ev_losses_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x9vtHxiOUVgU"
      },
      "outputs": [],
      "source": [
        "# plot all together.\n",
        "\n",
        "x_values = range(1, len(mp_baseline_m) + 1)\n",
        "\n",
        "plt.plot(x_values, mp_baseline_m * 100, label='MPLP trained')\n",
        "plt.plot(x_values, mp_stepf_m * 100, label='MPLP step function')\n",
        "plt.plot(x_values, mp_big_net_m * 100, label='MPLP on bigger network')\n",
        "plt.plot(x_values, sgd_m * 100, label='SGD')\n",
        "plt.plot(x_values, adam_m * 100, label='Adam')\n",
        "plt.xticks([4, 8, 12, 16, 20])\n",
        "\n",
        "#plt.ylim(0.0, 0.04)\n",
        "plt.xlabel(\"Number of steps\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "with open(\"tmp/mplp_evals.png\", \"wb\") as fout:\n",
        "  plt.savefig(fout)\n",
        "%download_file tmp/mplp_evals.png"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//experimental/selforg:selforg_colab",
        "kind": "private"
      },
      "name": "sharednet ms 4: L2L MNIST.ipynb",
      "provenance": [
        {
          "file_id": "1CfiYwYxmArmYc6xgyqH9S9_uDtQCQ_xA",
          "timestamp": 1593437800424
        },
        {
          "file_id": "14Gz-Rpnog-o4kk2hy_FltL6V7TzBGhZ3",
          "timestamp": 1589037587398
        },
        {
          "file_id": "16gXye-iMJYHdW8p43ibkRfCKr9l3Tuxv",
          "timestamp": 1589027248195
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
