<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
		<script type="text/javascript" src="dist/main.js"></script>
		<title>Differentiable Logic CA: from Game of Life to Pattern Generation</title>
		<link rel="apple-touch-icon" sizes="180x180" href="images/notaorb_180.png?b">
		<link rel="icon" type="image/png" sizes="32x32" href="images/notaorb_32.png?b">
		<link rel="icon" type="image/png" sizes="16x16" href="images/notaorb_16.png?b">
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-NXJWS6Y0G1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());
		  gtag('config', 'G-NXJWS6Y0G1');
		</script>
		<script src="./template.v2.js"></script>
		<script>
			  var circuit, monitor, monitorview, iopanel, paper;
    const loadCircuit = function (json) {
      circuit = new digitaljs.Circuit(json);
      paper = circuit.displayOn($('#paper'));
      circuit.start();
	  return circuit;
    };

    fetch('checkerboard.json')
    .then((response) => response.json())
    .then((json) => $(window).ready(function () { 
		
      let outcircuit = loadCircuit(json); 
      let inputcells = outcircuit.getInputCells()

      // set all of them randomly
      for (const element of outcircuit.getInputCells()){
        if(Math.random() < 0.5){
          element.toggleInput(); 
        }
      }

      let intervalId = setInterval(() => {
        let i = Math.floor(Math.random() * inputcells.length);
        let r = inputcells[i];
        r.toggleInput(); 
      },  1000);

	  $('#paper').on("click", () => clearInterval(intervalId));

    }));


</script>
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Differentiable Logic Cellular Automata: From Game of Life to pattern generation with learned recurrent circuits">
  <meta name="twitter:description" content="We use Differentiable Logic Gate Networks to create end-to-end differentiable, self-organizing discrete cellular automata powered by recurrent circuits, capable of playing the Game of Life as well as producing patterns à la Neural Cellular Automata.">
  <meta name="twitter:url" content="https://pietromiotti.github.io/diffLogicCA/">
  <meta name="twitter:image" content="https://pietromiotti.github.io/diffLogicCA/images/card.png">
  <meta name="twitter:image:width" content="560">
  <meta name="twitter:image:height" content="295">
<style>
#gcircuit {
    /* width: 400px; */
    background: url(images/g_logo_cropped.png) left top no-repeat;
    box-shadow: 25px 25px 50px 0 white inset, -25px -25px 50px 0 white inset;
    width: 400px;
    height: 400px;
    background-size: cover;
}
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.0;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin: 0 auto;
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}

.aff {
    font-size: 0.8em;
}
</style>

</head>
<body>
	<article id="17b0a320-00ae-80fd-b261-f8c156a456ad" class="page sans"><header><h1 class="page-title">Differentiable Logic Cellular Automata</h1><p class="page-description">From Game of Life to pattern generation with learned recurrent circuits</p></header><div class="page-body"><hr id="1a00a320-00ae-80de-a29e-dbb965692ffb"/><div id="1a00a320-00ae-80af-b579-fb795f51bafd" class="column-list"><div id="1a00a320-00ae-800d-8008-f5e37e6d0e83" style="width:100%" class="column"><p id="1a00a320-00ae-8056-8112-ca87a898c39e" class="aff">AUTHORS:</p><p id="1a00a320-00ae-80c4-aace-e3c77df45179" class="aff"><a href="https://pietromiotti.github.io/website/">Pietro Miotti</a></p><p id="1a00a320-00ae-806f-aad5-c243331975b3" class="aff"><a href="https://eyvind.me/">Eyvind Niklasson</a></p><p id="1a00a320-00ae-80dd-b747-e44115433cf7" class="aff"><a href="https://oteret.github.io/">Ettore Randazzo</a></p><p id="1a00a320-00ae-80c1-bb39-f3f675a69cee" class="aff"><a href="https://znah.net/">Alexander Mordvintsev</a></p></div><div id="1a00a320-00ae-80c0-b210-f83b699f92a0" style="width:100%" class="column"><p id="1a00a320-00ae-8011-b4ba-c28643baceb5" class="aff">AFFILIATIONS:</p><p id="1a00a320-00ae-805b-85c7-f25884be9e9d" class="aff">Google, Paradigms of Intelligence Team</p><p id="1a00a320-00ae-80d6-9420-e4dedc59f602" class="">
</p><p id="1a00a320-00ae-80a8-a013-ebde087612e4" class="">
</p></div><div id="1a00a320-00ae-8039-8725-e431f0a0dc9c" style="width:100%" class="column"><p id="1a00a320-00ae-805e-b43f-cc895105efa5" class="aff">PUBLISHED:</p><p id="1a00a320-00ae-808c-860d-fe4f0acb4098" class="aff">March 3, 2025</p><p id="1a00a320-00ae-807d-a880-f9998098d91e" class="">
</p></div></div><hr id="1a00a320-00ae-80bc-abd0-c2cdd760cf39"/><p id="1a00a320-00ae-80bb-a273-e899edfd6cf5" class="">Imagine trying to reverse-engineer the complex, often unexpected patterns and behaviors that emerge from simple rules. This challenge has inspired researchers and enthusiasts that work with cellular automata for decades.  In cellular automata, we generally approach things from the bottom-up. We choose local rules, then investigate the resulting emergent patterns. What if we could create systems that, given some complex desired pattern, can, in a fully differentiable fashion, learn the local rules that generate it, while preserving the inherent discrete nature of cellular automata? This is what we&#x27;ll  explore with you today.</p><div id="17c0a320-00ae-8050-84d9-eed83c749260" class="column-list"><div id="9f03a6f1-88e6-40f7-addb-635001c9dac0" style="width:50%" class="column"><figure id="17c0a320-00ae-8051-b45b-f1e017e41857" class="image"><div id="gcircuit" style="width:400px"></div><figcaption>Zoomed view of learned circuit</figcaption></figure><p id="1830a320-00ae-808f-ac51-db700b81c0b1" class="">
</p></div><div id="078f6020-e34b-49ff-ac61-550d1722c0df" style="width:50%" class="column"><figure id="17c0a320-00ae-80cd-a191-f479ea5492f4" class="image"><a href="images/google_logo.gif"><img style="width:400px" src="images/google_logo.gif"></a><figcaption>"G" being generated by learned circuit</figcaption></figure><p id="1830a320-00ae-80f8-9f9f-eefe3c827f4b" class="">
</p></div></div><p id="1910a320-00ae-80c4-a7d4-ee00e98e6b47" class="">Prior work has explored learning transition rules using non-differentiable techniques, demonstrating the feasibility of evolving local rules for specific computation<d-cite key="Mitchell1994-mi"></d-cite>. Likewise prior exploration of making one-dimensional cellular automata differentiable exist<d-cite key="Martin2017-dx"></d-cite>. We propose a novel, fully end-to-end differentiable approach, combining two interesting concepts from the world of artificial intelligence: Neural Cellular Automata (NCA) <d-cite key="Mordvintsev2020-oh"></d-cite> and Differentiable Logic Gates Networks <d-cite key="Petersen2022-ai"></d-cite><d-cite key="Petersen2024-rr"></d-cite>. NCA exhibit the ability to learn arbitrary patterns and behaviors, however, they do not inherently operate within a discrete state space. This makes interpretability more challenging, and leaves them stuck in a regime where current hardware must perform costly matrix multiplications to gradually update their continuous internal states. Differentiable Logic Gates Networks, meanwhile, have been used to discover combinatorial logic circuits, blending discrete states with differentiable training signals. But they haven&#x27;t yet been shown to work  in recurrent settings. NCA, as it were, are recurrent in both space, and time.  Sounds intriguing, right?</p><p id="17b0a320-00ae-80b3-8af8-f8d7b98ddb34" class="">Zooming out, we believe the integration of differentiable logic gates and neural cellular automata is a potential step towards programmable matter - <em>Computronium</em>  <d-cite key="Amato1991-ck"></d-cite> - a theoretical physical substance capable of performing arbitrary computation. Toffoli and Margolus pioneered this direction with  CAM-8, a cellular automata based computing architecture <d-cite key="Margolus1995-hg"></d-cite><d-cite key="Toffoli1991-do"></d-cite>, in theory capable of immense, horizontally scalable computation. However, they faced a fundamental challenge: actually crafting the local rules needed to achieve a desired macroscopic computation, with Amato et al. noting that “other researchers [...] still worry about the difficulty of finding local rules that correspond to real natural systems” <d-cite key="Amato1991-ck"></d-cite>.  What if we could directly learn these local rules, and create models that combine binary logic, the flexibility of neural networks, and the local processing of cellular automata?  We believe our prototypes  offer a glimpse into the future of computing: learnable, local, and discrete.</p><p id="17b0a320-00ae-8046-8ed4-e7cb4deb6da4" class="">This article will walk you through implementing cellular automata using differentiable logic gates, and demonstrate some key results along the way.</p><p id="1820a320-00ae-8036-b527-f6da83547f0a" class="">We&#x27;re faced with two fundamental questions.<em>                                      </em> </p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1820a320-00ae-800f-a24a-ec0ee8f611ec"><div style="font-size:1.5em"><span class="icon">❓</span></div><div style="width:100%"><p id="1820a320-00ae-80b1-9312-cf2feaaf9f38" class=""><em>Can a Differentiable Logic CA learn at all?   </em></p></div></figure><p id="1820a320-00ae-804b-bef7-d0f44f912fad" class="">To answer this, we&#x27;ll start by attacking Conway&#x27;s Game of Life - perhaps the most iconic cellular automata,  having captivated researchers for decades. While this first experiment might seem overly simplistic (functionally equivalent to learning a truth table), it will prove the basic learning capability of our setup. The more profound question follows: </p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1820a320-00ae-806a-8d96-d17225e588f1"><div style="font-size:1.5em"><span class="icon">❓</span></div><div style="width:100%"><p id="1820a320-00ae-8061-9b8f-c36fa2937fc1" class=""><em>Can recurrent-in-space and recurrent-in-time circuits learn complex patterns similar to those generated by traditional NCAs? </em></p></div></figure><p id="1990a320-00ae-804e-a1ae-fd0f1eca6c2c" class="">While both Differentiable Logic Gate Networks and Neural Cellular Automata (NCAs) have demonstrated trainability, effectively training circuits that exhibit both temporal and spatial recurrence of NCA, within the framework of differentiable logic, remains unexplored. </p><p id="1990a320-00ae-80ec-830f-c68734ecb12e" class="">The second experiment will demonstrate the model&#x27;s ability to learn recurrent circuits that generate complex patterns similar to the ones generated by traditional NCA. </p><h2 id="17b0a320-00ae-80e7-8662-c6ca0699315a" class="">Recap - Neural Cellular Automata</h2><p id="17b0a320-00ae-8094-8353-cbf1959b13bb" class="">At the heart of this project lies Neural Cellular Automata (NCA), a synthesis of classical cellular automata with modern deep learning techniques. This powerful paradigm, pioneered by Mordvintsev et al. <d-cite key="Mordvintsev2020-oh"></d-cite>, represents a fundamental shift in how we think about computational systems that can grow, adapt, and self-organize.</p><p id="17b0a320-00ae-8058-837d-d64466738811" class="">Traditional cellular automata have long captivated researchers with their ability to generate complex behaviors from simple, local rules. Neural Cellular Automata take this concept further by making these rules learnable through gradient descent. Instead of hand-designing update rules, the system discovers them automatically, opening up entirely new possibilities for self-organizing computational systems.</p><p id="17b0a320-00ae-80ce-a251-c52dd6986d31" class="">What makes this approach particularly elegant is how it preserves the core principles of cellular automata - locality, parallelism, and state-based computation - while introducing the adaptability of neural networks. </p><p id="17b0a320-00ae-802b-b083-ddbd44add2b7" class="">In the following sections, we will summarize the main concepts from the “<strong>Growing Neural Cellular Automata”</strong> work <d-cite key="Mordvintsev2020-oh"></d-cite>, which presents a Neural Cellular Automata developed for morphogenesis. If you are already familiar with it, feel free to skip it.</p><h3 id="17b0a320-00ae-8015-91a4-e90b65fe9a6c" class="">The Structure: A 2D Grid of Intelligent Cells</h3><p id="17b0a320-00ae-80ef-923a-d9cc324efe92" class="">At the heart of the system is a 2D grid, much like classic cellular automata. Each cell contains an <strong>n-dimensional vector</strong> of information which is called the cell&#x27;s <strong>state (or channels)</strong> and for the specific case of Growing-NCA it is composed by these elements:</p><ul id="17b0a320-00ae-8070-92fb-c1b3f696df56" class="bulleted-list"><li style="list-style-type:disc"><strong>RGB Colors</strong> (3 channels): These represent the visible properties of the cell, essentially its color.</li></ul><ul id="17b0a320-00ae-80ab-bdc9-f15b1fe22d1d" class="bulleted-list"><li style="list-style-type:disc"><strong>Alpha (α) Channel</strong> (1 channel): This indicates cell vitality. If the alpha value is greater than 0.1, the cell is considered “alive.”</li></ul><ul id="17b0a320-00ae-8001-88e8-eb07b7d7db40" class="bulleted-list"><li style="list-style-type:disc"><strong>Hidden Channels</strong> (n minus 4 channels): These allow cells to communicate more complex information about their environment, making interactions richer and more dynamic.</li></ul><p id="17b0a320-00ae-8053-886b-ddf0aadc026e" class="">But the magic doesn’t stop here. What really sets this system apart is how the cells interact and evolve through a two-stage process.</p><h3 id="17b0a320-00ae-80cb-9108-d89841539c77" class="">The Two-Stage Update Mechanism: Perception and Update</h3><p id="17b0a320-00ae-80e3-b748-d8ca6f3bd94a" class="">
</p><ol type="1" id="17b0a320-00ae-804c-a4a8-d1b14e54e541" class="numbered-list" start="1"><li><strong>The Perception Stage</strong><p id="17b0a320-00ae-80db-bdc0-f7258a4a6278" class="">In the first stage, each cell perceives its environment. Think of it as a cell sensing the world around it. To do this, it uses <strong>Sobel filters</strong>, mathematical tools designed to numerically approximate spatial gradients - &quot;changes across its surroundings&quot;. The filters are applied channel-wise, and the result is termed the <strong>perception vector</strong>, which combines the cell’s current state with the information it gathers about its environment. A bit like how biological cells use chemical gradients to sense and react to their surroundings.</p></li></ol><ol type="1" id="17b0a320-00ae-8088-acd2-c653be5101a3" class="numbered-list" start="2"><li><strong>The Update Stage</strong><p id="17b0a320-00ae-80de-9206-f48a8894d617" class="">Next, the neural network steps in. Each cell uses its perception vector as input to a neural network, which performs identical operations on every cell in the grid. Using around ~<strong>8,000 parameters</strong>, the neural network determines how each cell should change based on the information it has gathered. It’s here that the system evolves, with the cells adapting and responding to environmental changes.</p><p id="17b0a320-00ae-8059-b85f-df7a829ecc5b" class="">
</p><figure id="17b0a320-00ae-8098-80a0-e69315e4c00b" class="image"><a href="images/model.svg"><img style="width:679.578125px" src="images/model.svg"/></a><figcaption>Learning process for Growing NCA, image by Mordvintsev et al.<d-cite key="Mordvintsev2020-oh"></d-cite> </figcaption></figure><p id="17b0a320-00ae-80ee-9f34-e0b14498b76c" class="">
</p></li></ol><h3 id="17b0a320-00ae-80d5-9c81-ca1b38c6eef5" class="">The Power of Differentiability</h3><p id="17b0a320-00ae-803f-b0f6-e2eb73004110" class="">What makes this system truly powerful is its <strong>differentiability</strong>. Every operation, from perceiving the environment to updating its state, is fully differentiable. This means we can optimize the entire system through <strong>gradient descent</strong>, just like how neural networks learn from data. As a result, the system isn’t statically pre-defined with some arbitrary rules — it can actually <strong>learn</strong> to grow specific patterns or behaviors, making it a powerful tool for modeling complex systems.</p><figure id="1910a320-00ae-8027-a1a6-db5e3610fa63" class="image"><a href="images/regen2.gif"><img style="width:336px" src="images/regen2.gif"/></a><figcaption>NCA Growing process, credit to <d-cite key="Mordvintsev2020-oh"></d-cite> </figcaption></figure><p id="17b0a320-00ae-80c0-98fe-e15f2925d588" class="">While the individual components of the system (like Sobel filters and neural networks) are relatively simple, their combination creates something much more sophisticated. It’s a balance between simplicity and complexity, much like biological systems in nature, where local interactions lead to the emergence of surprising, intricate behaviors.</p><p id="17b0a320-00ae-8054-80ce-d80770602c35" class="">This approach doesn’t just push the boundaries of what cellular automata can do, it opens up a world of possibilities for learning, growth, and pattern formation through local interactions alone. Whether you’re a researcher, a developer, or simply someone fascinated by the intersection of AI and complexity, there’s a lot to explore here.  </p><p id="17b0a320-00ae-800a-87f3-f2bfda7a56e3" class="">Other applications of Neural Cellular Automata include Image Segmentation <d-cite key="Sandler2020-nx"></d-cite>, Image classification <d-cite key="Randazzo2020-mh"></d-cite> and many more. </p><div id="17c0a320-00ae-8082-9fe2-d7ba3a948c55" class="column-list"><div id="d21df3d1-8bd1-4991-a135-4d7a1ff782ee" style="width:68.75%" class="column"><h2 id="3c1f6714-1e60-4811-a242-1890db2faa19" class="">Recap - Differentiable Logic Gate Networks</h2></div><div id="dbd4084d-020e-40cb-a784-c232c19bfcea" style="width:31.25%" class="column"><p id="17c0a320-00ae-800b-840f-cb6928d74774" class="">
</p><p id="78885380-799d-499d-b070-fd22b378857a" class="">
</p></div></div><p id="17b0a320-00ae-8032-afd2-d1679ae2ad41" class="">What if we could take the basic building blocks of computation (logic gates like AND, OR, and XOR) and combine them in a learned fashion, to solve some task ? That&#x27;s exactly what <strong>Deep Differentiable Logic Gate Networks</strong> (DLGNs) achieve, merging the efficiency of digital circuits with the power of machine learning. This approach, developed by Petersen et al. <d-cite key="Petersen2022-ai"></d-cite><d-cite key="Petersen2024-rr"></d-cite>, opens up exciting possibilities, especially in resource-constrained environments like edge computing and embedded systems. </p><p id="17c0a320-00ae-809a-a485-cddf567b2c5a" class="">
</p><figure id="17c0a320-00ae-80e3-9e65-fe8301c314b1" class="image"><a href="images/diffLogicGateNetwork.png"><img style="width:400px" src="images/diffLogicGateNetwork.png"/></a><figcaption>Convolution Differentiable Logic Gate Network, image by Petersen et al.<d-cite key="Petersen2024-rr"></d-cite></figcaption></figure><h3 id="17b0a320-00ae-80a9-8959-fd52f3a68f89" class=""><strong>How Do Deep Differentiable Logic Gate Networks Work?</strong></h3><h3 id="17b0a320-00ae-803b-ab43-df550941a8e8" class=""><strong>Logic Gates as Neurons</strong></h3><p id="17b0a320-00ae-807a-ada3-f42de63afc1d" class="">At their core, DLGNs use <strong>logic gates</strong> as their building blocks, instead of the traditional artificial neurons found in neural networks. Each node in this case is a logic gate, and instead of performing weighted sums and matrix multiplications, each gate performs simple operations like <strong>AND, OR, XOR</strong>, etc.</p><h3 id="17b0a320-00ae-80fd-b4a9-d40ad5d9e6dc" class=""><strong>The Architecture: </strong></h3><p id="17b0a320-00ae-802a-9d09-ea4fc259be20" class="">The architecture of a DLGN is surprisingly simple:</p><ul id="17b0a320-00ae-80e0-88e6-f540f2557e3c" class="bulleted-list"><li style="list-style-type:disc">The network is composed of <strong>layers of gates</strong>. Each gate takes inputs from two gates in the previous layer, resulting in a naturally <strong>sparse </strong>network.</li></ul><ul id="17b0a320-00ae-8051-9e01-d3a37f22065d" class="bulleted-list"><li style="list-style-type:disc">The <strong>connections</strong> between gates are <strong>fixed</strong>; they are randomly initialized but do not change during training. The learning process determines what each gate does, not the connections between gates.</li></ul><ul id="17b0a320-00ae-80a2-98b4-d4d6224c522e" class="bulleted-list"><li style="list-style-type:disc"><strong>During inference</strong>, each gate performs one simple binary operation (think AND or OR) based on the operation it learned. </li></ul><h3 id="17b0a320-00ae-80ea-8efd-f37c705c5ee8" class=""><strong>The Learning Process: Making Discrete Operations Differentiable</strong></h3><p id="17c0a320-00ae-8005-a9e7-d933d0ca3d1a" class="">Instead of learning weights as traditional neural networks do, this network <strong>learns which logic operation each gate should perform</strong>. During training, each node solves a classification task to identify the correct gate to use in order to minimize the objective function.</p><p id="1a00a320-00ae-8075-958f-fad7dc45a4ba" class="">The challenge is that logic gates are inherently <strong>discrete</strong> and <strong>non-differentiable</strong>, making them unsuitable for gradient-based learning. So how do we make them learn? Through two key tricks:</p><ol type="1" id="17b0a320-00ae-80cc-b278-ca1bb0950836" class="numbered-list" start="1"><li><strong>Continuous Logic Operations</strong><p id="17b0a320-00ae-8006-99e9-e4703974fd0c" class="">During training, each logic operation is replaced by a continuous relaxation, which is a <strong>differentiable version</strong> that operates on continuous values between 0 and 1. For example, instead of a <em>hard</em> AND gate that only accepts 0 or 1, we use a <em>soft</em> AND gate that can handle values between 0 and 1 as inputs, and passes a continuous mix of the two inputs as its output. These continuous relaxations (listed below) allow us to train the network using <strong>gradient descent.</strong></p></li></ol><ol type="1" id="17b0a320-00ae-8058-bcc0-e1969485202e" class="numbered-list" start="2"><li><strong>Probabilistic Gate Selection</strong><p id="17b0a320-00ae-8038-aecd-d59a11a90eb6" class="">Each gate maintains a <strong>probability distribution</strong> over the 16 possible binary operations for two inputs. This distribution is represented by a 16-dimensional parameter vector, which is then transformed into a probability distribution using <em>softmax</em>. The values of the 16-dimensional vector are modified during the training process: over time, the gate <em>learns</em> to prefer one operation over others.</p></li></ol><table id="17c0a320-00ae-807b-96a5-fc5da4beace8" class="simple-table"><tbody><tr id="17c0a320-00ae-8019-b393-cb5454de12fe"><td id="&gt;zdq" class="" style="width:173px">Index</td><td id="AewD" class="" style="width:173px">Operation</td><td id="yM_;" class="" style="width:173px">Continuous Relaxation</td><td id="i&gt;w[" class="" style="width:173px">Symbol</td></tr><tr id="17c0a320-00ae-8051-a8a6-ea87f18d390e"><td id="&gt;zdq" class="" style="width:173px">0</td><td id="AewD" class="" style="width:173px">FALSE</td><td id="yM_;" class="" style="width:173px">0</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/FALSE.png"></img></td></tr><tr id="17c0a320-00ae-80fc-bd4d-efa3309483f2"><td id="&gt;zdq" class="" style="width:173px">1</td><td id="AewD" class="" style="width:173px">AND</td><td id="yM_;" class="" style="width:173px">a * b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/AND.png"></img></td></tr><tr id="17c0a320-00ae-80be-bc07-e0ee2bce83cb"><td id="&gt;zdq" class="" style="width:173px">2</td><td id="AewD" class="" style="width:173px">A AND (NOT B)</td><td id="yM_;" class="" style="width:173px">a - a*b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/AANDNOTB.png"></img></td></tr><tr id="17c0a320-00ae-8025-9ee0-fea1a0311050"><td id="&gt;zdq" class="" style="width:173px">3</td><td id="AewD" class="" style="width:173px">A</td><td id="yM_;" class="" style="width:173px">a</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/A.png"></img></td></tr><tr id="17c0a320-00ae-80f8-b4de-dec5453e190f"><td id="&gt;zdq" class="" style="width:173px">4</td><td id="AewD" class="" style="width:173px">(NOT A) AND B</td><td id="yM_;" class="" style="width:173px">b - a*b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/NOTAANDB.png"></img></td></tr><tr id="17c0a320-00ae-802e-8c3f-f80d00c69053"><td id="&gt;zdq" class="" style="width:173px">5</td><td id="AewD" class="" style="width:173px">B</td><td id="yM_;" class="" style="width:173px">b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/B.png"></img></td></tr><tr id="17c0a320-00ae-80f5-9a84-ed1adc3a7614"><td id="&gt;zdq" class="" style="width:173px">6</td><td id="AewD" class="" style="width:173px">XOR</td><td id="yM_;" class="" style="width:173px">a + b - 2a<em>*</em>b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/XOR.png"></img></td></tr><tr id="17c0a320-00ae-80bd-a29f-ea6d14a536c7"><td id="&gt;zdq" class="" style="width:173px">7</td><td id="AewD" class="" style="width:173px">OR</td><td id="yM_;" class="" style="width:173px">a + b - a*b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/OR.png"></img></td></tr><tr id="17c0a320-00ae-80eb-b6f9-e952034333bc"><td id="&gt;zdq" class="" style="width:173px">8</td><td id="AewD" class="" style="width:173px">NOR</td><td id="yM_;" class="" style="width:173px">1 - (a + b - a*b)</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/NOR.png"></img></td></tr><tr id="17c0a320-00ae-802f-a05e-ea21aded15f3"><td id="&gt;zdq" class="" style="width:173px">9</td><td id="AewD" class="" style="width:173px">XNOR</td><td id="yM_;" class="" style="width:173px">1 - (a + b - 2a*b)</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/XNOR.png"></img></td></tr><tr id="17c0a320-00ae-80b3-a49c-ebe99a8723f6"><td id="&gt;zdq" class="" style="width:173px">10</td><td id="AewD" class="" style="width:173px">NOT B</td><td id="yM_;" class="" style="width:173px">1 - b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/NOTB.png"></img></td></tr><tr id="17c0a320-00ae-80f8-9123-def2f0786f18"><td id="&gt;zdq" class="" style="width:173px">11</td><td id="AewD" class="" style="width:173px">A OR (NOT B)</td><td id="yM_;" class="" style="width:173px">1 - b + a*b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/AORNOTB.png"></img></td></tr><tr id="17c0a320-00ae-8045-9de1-cfd5a5c32556"><td id="&gt;zdq" class="" style="width:173px">12</td><td id="AewD" class="" style="width:173px">NOT A</td><td id="yM_;" class="" style="width:173px">1 - a</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/NOTA.png"></img></td></tr><tr id="17c0a320-00ae-80dc-970c-c24859f19813"><td id="&gt;zdq" class="" style="width:173px">13</td><td id="AewD" class="" style="width:173px">(NOT A) OR B</td><td id="yM_;" class="" style="width:173px">1 - a + a*b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/NOTAORB.png"></img></td></tr><tr id="17c0a320-00ae-8048-aa3f-f188631922c8"><td id="&gt;zdq" class="" style="width:173px">14</td><td id="AewD" class="" style="width:173px">NAND</td><td id="yM_;" class="" style="width:173px">1 - a*b</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/NAND.png"></img></td></tr><tr id="17c0a320-00ae-80d2-8d65-ec7ff8d46314"><td id="&gt;zdq" class="" style="width:173px">15</td><td id="AewD" class="" style="width:173px">TRUE</td><td id="yM_;" class="" style="width:173px">1</td><td id="i&gt;w[" class="" style="width:173px"><img src="images/TRUE.png"></img></td></tr></tbody></table><p id="17c0a320-00ae-8070-b380-c442ec22ce0a" class="">During <strong>training</strong>, the network uses the continuous relaxations of the logic operations, but once the network is trained, we switch to <strong>pure binary operations</strong> for lightning-fast inference.<div class="indented"><figure id="17c0a320-00ae-807d-a43c-f2e551e9acf8" class="image"><a href="images/ed21e753-3109-4d2d-9df2-9c4d7a037e61.png"><img style="width:672px" src="images/ed21e753-3109-4d2d-9df2-9c4d7a037e61.png"/></a><figcaption>Sketch illustrating the training of a single Gate</figcaption></figure></div></p><p id="17b0a320-00ae-80d2-8e9c-c28c10d4b78e" class="">To facilitate training stability, the initial distribution of gates is biased toward the <em>pass-through</em> gate.</p><h3 id="17b0a320-00ae-801d-a985-f6758054bec4" class=""><strong>Training: </strong><strong>L</strong><strong>earning the Gates</strong></h3><p id="17b0a320-00ae-80c1-864f-fa24481561eb" class="">The training process follows a standard forward-backward pass:</p><ol type="1" id="17b0a320-00ae-80f4-9cbb-f210c1ed6a82" class="numbered-list" start="1"><li><strong>Forward Pass</strong><ul id="17b0a320-00ae-8081-8f58-ee7638036e4c" class="bulleted-list"><li style="list-style-type:disc">The input values propagate through the network.</li></ul><ul id="17b0a320-00ae-80bd-8e52-cf1e8133140f" class="bulleted-list"><li style="list-style-type:disc">Each gate, given two inputs, computes the results of all 16 possible logic operations using their continuous relaxations.</li></ul><ul id="17b0a320-00ae-803a-94f0-e1c705058759" class="bulleted-list"><li style="list-style-type:disc">These results are weighted according to the gate’s probability distribution, and the <strong>weighted sum</strong> becomes the output of the gate.</li></ul></li></ol><ol type="1" id="17b0a320-00ae-8061-b44d-f961bceae50a" class="numbered-list" start="2"><li><strong>Backward Pass</strong><ul id="17b0a320-00ae-80d2-8ffe-e3e960d4d715" class="bulleted-list"><li style="list-style-type:disc">The network computes the <strong>gradients</strong> with respect to the probability distributions, which are then updated using <strong>gradient descent</strong>.</li></ul><ul id="17b0a320-00ae-8050-aed2-d26278b130aa" class="bulleted-list"><li style="list-style-type:disc">Over time, each gate’s distribution becomes more squeezed and spontaneously converge on one operation, whether it’s AND, OR, XOR, or another.</li></ul></li></ol><h3 id="17b0a320-00ae-8043-8fec-f86cfb47debb" class=""><strong>Inference: The Magic of Binary Operations</strong></h3><p id="17b0a320-00ae-80c9-a69a-d6e102d0e685" class="">Once training is complete, we can freeze the network. This means that each gate <strong>settles on its most probable operation</strong>, and the continuous versions of the logic operations are discarded. What’s left is a <strong>pure logic circuit</strong> that operates on binary values (0 or 1).</p><p id="17b0a320-00ae-80fb-809b-c3220e533e6f" class="">This final form is incredibly efficient. When it’s time to deploy, the network runs using only <strong>binary operations</strong>, making it exceptionally fast on any hardware.</p><h2 id="17b0a320-00ae-8021-8f9d-eb9f303ac034" class="">Differentiable Logic Cellular Automata</h2><p id="17c0a320-00ae-808e-8573-fdd175f0ac57" class="">The integration of differentiable logic gate networks with neural cellular automata provides a solution for handling discrete states while maintaining differentiability. </p><p id="1a80a320-00ae-804b-abf1-c0ba0aaf46a6" class="">Let&#x27;s explore this system in depth, examining how it differs from traditional Neural Cellular Automata, while highlighting their common principles and understanding the fundamental role of differentiable logic gates. We&#x27;ll borrow the terminology of NCA stages, highlighting where our model differs.</p><h3 id="17b0a320-00ae-80d8-853f-e6d5a9fcb909" class="">The Structure: A 2D Grid of binary, intelligent cells</h3><p id="1960a320-00ae-80c7-b58b-d5e6c567ca01" class="">As with NCA, the system is built around a 2D grid of cells, where each cell&#x27;s state is represented by an <strong>n-dimensional binary vector</strong> This binary state vector acts as the cell&#x27;s working memory, storing information from previous iterations. Throughout this article, <em>cell state</em> and <em>channels</em> will be used interchangeably.</p><h3 id="17b0a320-00ae-80c6-b0da-fcc71275aac2" class="">The Two-Stage Update Mechanism: Perception and Update</h3><p id="17b0a320-00ae-805a-aa4d-e8222cd4a7ce" class="">
</p><ol type="1" id="17b0a320-00ae-8034-ab51-ff1b4ac57cb3" class="numbered-list" start="1"><li><strong>The </strong><strong>Perception</strong><strong> Stage</strong><div id="17c0a320-00ae-8061-a387-ecb4b1d2d84f" class="column-list"><div id="76bb245b-f562-4640-b34a-75e3bd6f4507" style="width:50%" class="column"><p id="17c0a320-00ae-80c8-afeb-d3933761f7ad" class="">In cellular automata systems, each cell must be aware of its environment. While traditional NCA use Sobel filters to model this perception, DiffLogic CA takes a different approach, following <d-cite key="Petersen2024-rr"></d-cite>. Each kernel is a distinct circuit, where connections are fixed with a particular structure, but the gates are learned. The kernels are computed channel-wise. Each circuit employs four layers whose connections are designed to compute  <em>interactions</em> between the central cell and its neighboring cells as in the figure on the right. The output dimension is the number of kernels multiplied by the number of channels. Alternative approaches involve kernels with multiple bits of output per channel, rather than only one, improving convergence in some cases.</p><p id="1820a320-00ae-8093-b505-d752f420c698" class="">
</p></div><div id="9a000272-c6fe-4248-972e-6ec94af6e057" style="width:50%" class="column"><p id="17c0a320-00ae-8035-90f0-d69701b7c016" class="">
</p><figure id="1820a320-00ae-80a0-a05d-fc29118859d4" class="image"><img style="width:350px"  src="images/perceive_(4).svg"><figcaption>Each kernel operates channel-wise and computes the interaction between the central cell and its neighbors, emulating the CA&#x27;s interaction within the Moore neighborhood. This 3x3 patch shows a state dimension of 3. The circuit is wired to process interactions between the central cell and its surrounding cells. The first layer has 8 gates, with each gate taking the central cell as its first input and a neighboring cell as its second input.</figcaption></figure></div></div></li></ol><ol type="1" id="17b0a320-00ae-8055-967a-dbcb60fd9aa9" class="numbered-list" start="2"><li><strong>The Update Stage</strong><div id="17c0a320-00ae-80f0-a694-ed77a59540bc" class="column-list"><div id="9a9dc48d-e2d6-4bc8-aae3-4a36879bf8fb" style="width:50%" class="column"><p id="17c0a320-00ae-80dd-b91e-e4455981461a" class="">
</p><p id="6d38c7a5-9c98-4b70-84df-074f0cbf26e1" class="">The update mechanism follows the NCA paradigm, but employs a Differentiable Logic Network to compute each cell&#x27;s new state. The network&#x27;s connections can be either randomly initialized or specifically structured to ensure all inputs are included in the computation. The updated state is determined by applying a Differentiable Logic Gate Network to the concatenation of the cell&#x27;s previous memory (represented in gray), and the information received from its neighbors (represented in orange). In standard NCA, at this point, one would incrementally update the state, treating the whole system like an ODE. With DiffLogic CAs, we output the new state directly.  </p></div><div id="c392a3cb-bd32-4a40-9cd3-668148e146de" style="width:50%" class="column"><figure id="1820a320-00ae-807f-bb72-f0fe7f137292" class="image"><img style="width:350px"  src="images/update_(2).svg"><figcaption>A representation of the update step given a cell state of dimension 4 and 2 kernels.</figcaption></figure></div></div></li></ol><p id="1a00a320-00ae-80cb-822b-c86cdc13c579" class="">In summary: the perception phase uses a logic gate network to process the binary neighborhood states, replacing traditional convolution filter-based operations, and the update rule is implemented as another logic gate network that takes the perception output and current state as inputs, and outputs the next binary state of the cell.</p><p id="1a00a320-00ae-8029-8c42-dad6b2b75a6f" class="">
</p><figure id="1a00a320-00ae-80d6-9ac9-c9b7651dc6e6" class="image" style="text-align:center"><img style="width:380px" src="images/cell_architecture.svg"><figcaption>Schematic representation of a 4x4 DiffLogic CA grid. At each time step, each cell reads and processes the information stored in its neighboring cells&#x27; states and then updates its own state</figcaption></figure><p id="1a00a320-00ae-80e2-bec3-e62e831ba9ff" class=""> </p><p id="1a00a320-00ae-807d-a0d8-cd4ede77f9c4" class="">The diagram above schematically represents a 4x4 DiffLogic CA grid,  each of the small squares is a tiny computer with a dual-memory system. We visualize these two registers as gray and orange, respectively. Every cell in our grid performs a two-step process, which we will later see can be either performed synchronously, or in some cases asynchronously: </p><ol type="1" id="1a00a320-00ae-80cf-95c1-d27d65e00a0d" class="numbered-list" start="1"><li>Step 1: The Perception Phase<br/>First, every cell in our grid becomes a data gatherer. They examine their neighbors&#x27; gray registers, process what they observe, and store their results in their orange registers.<br/></li></ol><ol type="1" id="1a00a320-00ae-8032-90c0-f862ce0748d1" class="numbered-list" start="2"><li>Step 2: The Update Phase<br/>Right after that, each cell becomes a decision maker. Using the information stored in both its registers (the original gray one and the newly filled orange one), the cell calculates its new state. This new state gets written to the gray register, while the orange register is cleared, ready for the next round of perception.<br/></li></ol><p id="1a00a320-00ae-80f3-8b64-dd80800837d7" class="">The system behaves like a network of tiny, independent computers that communicate with their neighbors and make decisions based on their observations. Each cell is  a miniature processor in a vast, interconnected grid, working together to perform complex computations through these simple local interactions. By combining local connections with distributed processing, we&#x27;ve built something that can tackle tasks exploiting the emergence of collective behavior. </p><p id="1a00a320-00ae-80ca-a49e-efb4665081d4" class="">We again find strong kinship with the the work <em>Programmable Matter </em>and <em>Computronium</em> by Toffoli and Margolus, who proposed the CAM-8 <d-cite key="Margolus1995-hg"></d-cite><d-cite key="Toffoli1991-do"></d-cite>, a computer architecture based on cellular automata which is similar to the system above where each cell uses a DRAM chips for state variables and an SRAM for processing.</p><figure id="1a00a320-00ae-80f3-92e0-c43cb1668296" class="image"><a href="images/CAM-8.png"><img style="width:679.9921875px" src="images/CAM-8.png"/></a><figcaption>Cam-8 architecture and image from from Margolus et al.<d-cite key="Margolus1995-hg"></d-cite></figcaption></figure><h2 id="17c0a320-00ae-8043-af6b-d7667381714c" class="">Experiment 1: Learning Game of Life</h2><div id="17c0a320-00ae-8036-975b-df6ed2216f95" class="column-list"><div id="6ecea7c3-689a-4597-b6a9-03aae07f9b1f" style="width:50%" class="column"><p id="17c0a320-00ae-802c-a613-c6c487269842" class="">
</p><p id="12744ce8-f2bc-4573-9602-f3f251460b90" class="">Conway&#x27;s Game of Life is a fascinating mathematical simulation that demonstrates how complex patterns can emerge from simple rules. Created by mathematician John Conway in 1970, this <em>game</em> isn&#x27;t played in the traditional sense - it&#x27;s a cellular automaton where cells on a grid live or die based on just four basic rules. Despite its simplicity, the Game of Life can produce amazing behaviors, from stable structures to dynamic patterns that seem to take on a life of their own.</p><p id="17c0a320-00ae-8053-aa4d-f1ccac56c156" class="">
</p></div><div id="5732a09e-ef35-4104-b416-0f7aafae68de" style="width:50%" class="column"><p id="51507e0f-acce-4b75-b4fd-3d854defd82b" class="">
</p><figure id="17c0a320-00ae-804f-8698-d5b06774ecdf" class="image"><a href="images/game_of_life_simulation.gif"><img style="width:250px" src="images/game_of_life_simulation.gif"/></a><figcaption>Simulation of Conway&#x27;s Game of Life</figcaption></figure></div></div><p id="17c0a320-00ae-8092-b9ea-ed2c070ba6b0" class="">The rules of the game are elegantly simple, focusing on how each cell interacts with its eight neighboring cells:</p><ol type="1" id="17c0a320-00ae-804d-85aa-d9526a7e68af" class="numbered-list" start="1"><li><em>Birth</em>: A dead cell (whose current value is 0) with exactly three living neighbors springs to life in the next generation, as if by reproduction.</li></ol><ol type="1" id="17c0a320-00ae-8006-930a-c889e1f40943" class="numbered-list" start="2"><li><em>Survival</em>: A living cell (whose current value is 1) with either two or three living neighbors survives to the next generation, representing a balanced environment.</li></ol><ol type="1" id="17c0a320-00ae-8077-b3bd-f32ad09cc8e6" class="numbered-list" start="3"><li><em>Underpopulation</em>: A living cell with fewer than two living neighbors dies from isolation in the next generation.</li></ol><ol type="1" id="17c0a320-00ae-80e3-a940-c61f42e07b6b" class="numbered-list" start="4"><li><em>Overpopulation</em>: A living cell with more than three living neighbors dies from overcrowding in the next generation.</li></ol><p id="17c0a320-00ae-808c-8056-fd1db396f4d4" class="">These four rules, applied simultaneously to every cell in the grid at each step, create a dance of patterns. From these basic interactions emerge complex behaviors: <em>stable structures</em> that never change, <em>oscillators</em> that pulse in regular patterns, and even <em>gliders</em> that appear to move across the grid. It&#x27;s this emergence of complexity from simplicity that has made the Game of Life a powerful metaphor for self-organization in natural systems, from biological evolution to the formation of galaxies.</p><p id="17c0a320-00ae-8087-89e5-caa7c074f76e" class="">Given its binary and dynamic nature, Game of Life is a good sanity check of the DiffLogic CA. </p><h3 id="17c0a320-00ae-8061-a777-eeca554601ce" class="">State and Parameters</h3><p id="17c0a320-00ae-8043-915b-d32ea04bcf22" class="">Given that we know the rules are independent of previous state iterations, we consider a cell state consisting of 1 bit, meaning the system is essentially memory-less.  The model architecture includes 16 perception circuit-kernels, each of them with the same structure of nodes [8, 4, 2, 1]. The update network instead has 23 layers: first 16 layers have 128 nodes each, and the subsequent layers have [64, 32, 16, 8, 4, 2, 1] nodes, respectively.</p><h3 id="17c0a320-00ae-8050-b070-c81a2f73bfea" class="">Loss function</h3><p id="17c0a320-00ae-80a6-8fa8-d9dff565d888" class="">The loss function is computed by summing the squared differences between the predicted grid and the ground truth grid.</p><figure id="17c0a320-00ae-8006-baad-fd12853ac1a2" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i,j}^N(y_{i,j} - \tilde{y}_{i,j})^2  </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1502em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><h3 id="17c0a320-00ae-800e-8361-d0ed4714fa5e" class="">Training Dataset</h3><p id="17c0a320-00ae-8007-8251-fc1ae5ea91e5" class="">The model was trained on 3x3 periodic grids for a single time step. Given that each cell in the Game of Life interacts with its eight neighbors, and its next state is determined by its current state and the states of its neighbors, there are 512 possible unique configurations for a 3x3 grid. To train the model, we constructed a grid including all 512 possible grid configurations. Learning the next state of grid correctly implies learning the complete Game of Life rule set. The trained parameters were subsequently used to simulate the model&#x27;s behavior on larger grids.</p><h3 id="17c0a320-00ae-80ef-b8b5-ceb5e36e8b3e" class=""><strong>Results</strong></h3><p id="17c0a320-00ae-80d3-88f2-cef619c26726" class="">On the left, you can observe the loss plot comparing the two representations of logic gates. The <em><mark class="highlight-blue"><strong>soft</strong></mark></em><mark class="highlight-blue"><strong> loss</strong></mark> computes the output of the gates using their continuous approximation as explained in the previous section, while the <em><mark class="highlight-red"><strong>hard</strong></mark></em><em><mark class="highlight-red"><strong> loss</strong></mark></em> selects only the most probable gate and uses its discrete output. Both losses fully converge, indicating that we were able to generate a circuit that perfectly simulates the Game of Life.</p><p id="1990a320-00ae-80a8-83cc-f84133c2d11e" class="">Using hard inference (selecting most probable gates), the simulation on the right displays the learned circuit&#x27;s performance on a larger grid. The emergent patterns capture the structures from Conway&#x27;s Game of Life: gliders moving across the grid, stable blocks remaining fixed in place, and classic structures like loaves and boats maintaining their distinctive shapes. The successful replication of Game of Life&#x27;s characteristic patterns demonstrates that our circuit has effectively learned the underlying local rules.</p><p id="17c0a320-00ae-80eb-a86c-fecd26c0a734" class="">
</p><div id="17c0a320-00ae-8023-8d93-ca1fe9b614cc" class="column-list"><div id="6374f26a-1ff6-4c15-a6cb-288b0dfb7aff" style="width:50%" class="column"><figure id="1980a320-00ae-8061-85ea-f591ef1029d7" class="image"><a href="images/gof_loss.svg"><img style="width:400px" src="images/gof_loss.svg"/></a><figcaption>Training plot for DiffLogic CA learning Game of Life</figcaption></figure><p id="19a0a320-00ae-8090-9b51-d9cd5a8f059c" class="">
</p></div><div id="c01e6682-51f8-4834-9291-d86c414afc83" style="width:50%" class="column"><figure id="17b0a320-00ae-806b-955e-daf04e669128" class="image"><a href="images/gif_game_of_life.gif"><img style="width:250px" src="images/gif_game_of_life.gif"/></a><figcaption>Game of Life simulated by the learned circuit</figcaption></figure><p id="17c0a320-00ae-8019-b5ca-f8f43f6e410f" class="">
</p></div></div><h3 id="19a0a320-00ae-8063-bd2d-c699f4db50e0" class="">Analysis of the Generated Circuit </h3><p id="19a0a320-00ae-80af-8991-f65fef5f2047" class="">While circuit optimization is not the primary focus of this project, this section provides a brief analysis of the generated circuit.</p><p id="17c0a320-00ae-8066-884e-f2b96588559f" class="">The total number of <em>active</em> gates used (excluding the pass-through gates A and B), is 336. Examining the gate distributions, we observe that the most frequently used gates in both networks are OR and AND.</p><div id="17c0a320-00ae-809c-a901-f9f549c12a65" class="column-list"><div id="0afde922-8832-4e21-ab5e-42145d20e9a1" style="width:50%" class="column"><figure id="1980a320-00ae-80c8-8596-f369c0c1653b" class="image"><a href="images/gof_perceive_gates.svg"><img style="width:400px" src="images/gof_perceive_gates.svg"/></a><figcaption>Distribution of gate counts across all perception kernel circuits</figcaption></figure></div><div id="83be1920-a5a6-4bbd-812d-b1d1a1a4cc3d" style="width:50%" class="column"><figure id="1980a320-00ae-809c-add9-da7f172f103a" class="image"><a href="images/gof_update_gates.svg"><img style="width:400px" src="images/gof_update_gates.svg"/></a><figcaption>Distribution of gate counts across the update circuit.</figcaption></figure></div></div><p id="1a70a320-00ae-8064-a718-f9867702f336" class="">Since our final circuit is simply a series of binary gates, we can step even deeper and visualize the entirety of the circuit logic involved! Below is a visualization of most of these 336 gates (some gates are pruned, when we determine they don’t contribute to the output). </p><figure id="1a70a320-00ae-8057-81b6-d5a09ba88fb3" class="image"><a href="images/gol_circuit_full.png"><img style="width:8000px" src="images/gol_circuit_full.png"/></a><figcaption>Complete learned perceive-update circuit implementing Game of Life (<a href="gol.html">available interactively</a>)</figcaption></figure><p id="1a70a320-00ae-801a-a4f4-f2b54a03f311" class="">The squares arranged in a three-by-three grid on the left are the input gates, arranged as they would be when viewed from the perspective of a single, central, cell somewhere in the game of life. The wires are colored green when high (1), and red when low (0). Finally, each gate should be somewhat self-explanatory, being one of AND, OR or XOR gates, with small circles on inputs or on outputs to denote NOTs on those particular connections. We&#x27;ve additionally replaced the binary NotB and NotA gates with a unary Not gate, and pruned the unused input, to simplify visualization. Finally, some gates are simply “True” or “False”, and these look almost identical to the inputs, appearing as nested squares, either filled in (True) or empty (False).  </p><p id="1a70a320-00ae-800f-ae77-e6586a7d4263" class="">On the far right, we see the single output channel of this circuit - denoting the new state of the cell in the Game of Life. In this particular configuration in the figure, we see the circuit correctly computing the rule “Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.”</p><p id="1a70a320-00ae-800c-8cdb-f6a07a46a6dd" class="">We encourage readers to <a href="gol.html">directly interact with the circuit</a><d-cite key="MaterzokUnknown-wb"></d-cite>.</p><h2 id="17c0a320-00ae-806e-83c8-cf2856b2b387" class="">Experiment 2: Pattern Generation</h2><p id="17c0a320-00ae-80a8-a075-f0f39993e28b" class="">Neural Cellular Automata (NCA) have shown remarkable capabilities in pattern generation tasks <d-cite key="Mordvintsev2020-oh"></d-cite>, inspiring us to explore similar capabilities with diffLogic CA. In this task, the system evolves from a random initial state toward a target image, allowing multiple steps of computation. By evaluating the loss function only at the final time-step, we challenge the model to discover the discrete transition rules that guide the system through a coherent sequence of states without step-by-step supervision. </p><p id="1a80a320-00ae-8026-89b3-d3c8f76f6740" class=""><br/>Successfully learning to reconstruct images would validate two key aspects: the model&#x27;s ability to develop meaningful long-term dynamics through learned rules, and its capability to effectively learn <em>stateful, recurrent-in-time, recurrent-in-space</em> circuits. This investigation is particularly significant as it represents, according to the best of our knowledge, the first exploration of differentiable logic gate networks <d-cite key="Petersen2022-ai"></d-cite><d-cite key="Petersen2024-rr"></d-cite> in a recurrent setting.</p><h3 id="17c0a320-00ae-8083-8659-e20f9a41611e" class="">State and Parameters</h3><p id="17c0a320-00ae-80a3-8347-cb14d7db4b62" class="">We consider a cell state (channels) of 8 bits and iterate the DiffLogic CA for 20 steps. The model architecture includes 16 perception circuit-kernels, each with 8, 4, then  2 gates per layer, respectively. The update network has 16 layers: 10 layers with 256 gates each, and then layers with [128, 64, 32, 16, 8, 8] gates, respectively. </p><h3 id="17c0a320-00ae-801f-ab3f-ead7589b668c" class="">Loss function</h3><p id="17c0a320-00ae-80a4-80cd-e1cf54b12061" class="">We define the loss function as the sum of the squared differences between the first channel in the predicted grid and the target grid at the last time step.</p><figure id="17c0a320-00ae-8098-a736-e918eb6b04da" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mn>0</mn></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mn>0</mn></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i,j}^N(y_{i,j,0} - \tilde{y}_{i,j,0})^2  </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1502em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><h3 id="17c0a320-00ae-801c-a912-f98499dc9d47" class="">Training Dataset</h3><div id="17c0a320-00ae-8063-aba4-d927930e8cd1" class="column-list"><div id="41027de1-db12-49a7-9b8b-a2e7fe1c3892" style="width:50%" class="column"><p id="17c0a320-00ae-8012-a208-d2be1d27409c" class="">The model was trained to reconstruct a 16x16 checkerboard pattern within 20 time steps. For each training step, the initial state was randomly sampled. The target checkerboard pattern is shown on the right.</p><p id="17d0a320-00ae-807c-831f-cc9ed8435389" class="">
</p></div><div id="b2f4fe42-1f3c-4dc2-94f2-efc12224d5be" style="width:50%" class="column"><figure id="17c0a320-00ae-8048-a627-cbcd662de7ee" class="image"><a href="images/train_set.png"><img style="width:250px" src="images/train_set.png"/></a><figcaption>Target pattern</figcaption></figure></div></div><h3 id="17c0a320-00ae-80ed-b462-f42a3ae1680b" class=""><strong>Results</strong></h3><p id="17c0a320-00ae-807e-8245-c331ebdc614d" class="">The DiffLogic CA fully converges to the target pattern. The training plot (left) reveals consistent convergence of both soft and hard loss functions. The evolution of the first channel (right), which is used for computing the loss function, shows clear pattern formation. An intriguing emergent property is the directional propagation of patterns from bottom-left to top-right, despite the model having no built-in directional bias.</p><div id="1910a320-00ae-8066-bd37-df2342c6feb9" class="column-list"><div id="1910a320-00ae-80a8-ad71-e34a62674acc" style="width:50%" class="column"><figure id="17c0a320-00ae-80ba-af4c-fad9791b0ab9" class="image"><a href="images/checkerboard_loss.svg"><img style="width:400px" src="images/checkerboard_loss.svg"/></a><figcaption>Training plot for DiffLogic CA</figcaption></figure><p id="1910a320-00ae-8014-9c8a-c1356567baaa" class="">
</p></div><div id="1910a320-00ae-80fd-8d01-f41c9c542454" style="width:50%" class="column"><figure id="1900a320-00ae-80ab-9996-c2f3bb4703ae" class="image"><a href="images/checker_board_small.gif"><img style="width:250px" src="images/checker_board_small.gif"/></a><figcaption>Evolution of the diffLogic CA, only considering the first bit in the cell state. </figcaption></figure></div></div><h3 id="17c0a320-00ae-8029-84c0-c0a04827a153" class="">Analysis of the Generated Circuit</h3><p id="17c0a320-00ae-80dc-bddf-e75bba1ae1e5" class="">The total number of active gates used (excluding pass-through gates A and B) is 22. Analysis of the learned logic gates reveals a different distribution of gates between the perception kernels and update networks. The TRUE gate appears to play a key role in perception but not in the update one. </p><div id="17c0a320-00ae-8050-84d9-eed83c749260" class="column-list"><div id="9f03a6f1-88e6-40f7-addb-635001c9dac0" style="width:50%" class="column"><figure id="17c0a320-00ae-8051-b45b-f1e017e41857" class="image"><a href="images/checkerboard_sync_perceive_gates.svg"><img style="width:400px" src="images/checkerboard_sync_perceive_gates.svg"/></a><figcaption>Distribution of gate counts across all perception kernel circuits</figcaption></figure><p id="1830a320-00ae-808f-ac51-db700b81c0b1" class="">
</p></div><div id="078f6020-e34b-49ff-ac61-550d1722c0df" style="width:50%" class="column"><figure id="17c0a320-00ae-80cd-a191-f479ea5492f4" class="image"><a href="images/checkerboard_sync_update_gates.svg"><img style="width:400px" src="images/checkerboard_sync_update_gates.svg"/></a><figcaption>Distribution of gate counts across the update circuit.</figcaption></figure><p id="1830a320-00ae-80f8-9f9f-eefe3c827f4b" class="">
</p></div></div><p id="1a80a320-00ae-804f-bb69-c8a252e2d836" class="">Below, we provide an interactive visualization of the circuit, after pruning. Remarkably, we are left with just six gates - one of which is redundant - an <em>AND</em> between the same input. In other words; the entirety of the procedural checkerboard-generation function learned by the circuit can be implemented using just five logic gates. Likewise, most of the inputs and outputs remain unused. Even more remarkably, the cell&#x27;s own current visual output isn&#x27;t even considered in an update step. We encourage readers to interact with the circuit below <d-cite key="MaterzokUnknown-wb"></d-cite>, clicking on and off inputs on the left to observe the effect on the outputs.</p><p id="1a80a320-00ae-8042-97c6-d8ee3bc1d024" class="">
<p style="width:100%" class="column">
<figure class="image">
<div id="paper" style="width:100%" class="column">
</div>
<figcaption>Complete learned perceive-update circuit generating checkerboard, interactive</figcaption>
</figure>
</p>
<p style="width:100%" class="column">
<div id="monitor" style="width:100%" class="column">
</div></p>

</p></div></div><h3 id="1900a320-00ae-8093-9c21-fa0096e85afe" class="">How general is the solution? </h3><p id="1910a320-00ae-8063-b6f3-c3712abdae01" class="">To the naked eye, our solution appears to build the grid iteratively—brick by brick, as it were. However, during training, we only employed one fixed size of the grid. Naturally, we should investigate what happens if we change the grid size: is the rule we learned truly an iterative, procedural solution, or is it overfit to one particular grid size? Let&#x27;s scale up both the spatial and temporal dimensions by a factor of four—using a grid four times larger and running it for four times as many steps.</p><p id="1900a320-00ae-8036-9993-edf17314a3af" class="">
</p><figure id="1900a320-00ae-80e2-a164-f1c5a41da8f5" class="image"><a href="images/checker_board_large.gif"><img style="width:400px" src="images/checker_board_large.gif"/></a><figcaption>Generalization Test: Learned Rules Applied to 4x Larger Grid</figcaption></figure><p id="1900a320-00ae-8012-8000-d24e45e6528c" class="">Success! The circuit works just as well in this new setting. This raises an interesting question as to the inductive biases of this model. In the NCA setting, it was possible to coax behavior invariant to grid size and time, but this required either special spatially invariant loss functions <d-cite key="Niklasson2021-vb"></d-cite> , and in the case of the growing lizard a special &quot;alive/dead&quot;<d-cite key="Mordvintsev2020-oh"></d-cite> regime to prevent overfitting to boundary conditions. Here, our boundary conditions are also fixed, yet the model has learned a &quot;boundary-size-invariant&quot; way to produce the pattern. Could the discretization and minimal circuit size be finding some minimal procedural description for generating patterns of interest? </p><p id="1910a320-00ae-80b6-9642-fda8db6a59db" class="">Given our setting, we tested the system&#x27;s resilience to damage and its recovery capabilities through two experiments. In the first test (left), we evaluated pattern reconstruction when a large portion of cells were permanently disabled, simulating faulty components. In the second test (right), the disabled cells were reactivated after a set number of steps. The system demonstrated robust behavior in both scenarios: maintaining pattern integrity despite permanent cell damage in the first case, and successfully self-repairing to produce the correct pattern once damaged cells came back online in the second case.</p><p id="1980a320-00ae-8063-992e-c777deb8bbd4" class="">
</p><div id="1980a320-00ae-8042-b114-ed9a666621b3" class="column-list"><div id="1980a320-00ae-80af-bc4c-d2686c61e971" style="width:50%" class="column"><figure id="1900a320-00ae-8085-b5a4-c7f19fffa6b7" class="image"><a href="images/checker_board_fault.gif"><img style="width:400px" src="images/checker_board_fault.gif"/></a><figcaption>Fault-tolerance behaviour</figcaption></figure></div><div id="1980a320-00ae-80c7-ab81-dba691e8aeb3" style="width:50%" class="column"><figure id="1900a320-00ae-806c-9be3-d3fb5304e664" class="image"><a href="images/board_checkers_self_healing.gif"><img style="width:400px" src="images/board_checkers_self_healing.gif"/></a><figcaption>Self-healing behaviour.</figcaption></figure></div></div><p id="1980a320-00ae-806d-99ce-e3867d408cbd" class="">
</p><h3 id="1980a320-00ae-800d-bdc1-e5bf455b1d0e" class="">DiffLogic CA as new paradigm for robust computing </h3><p id="1980a320-00ae-80f4-a198-c9d7fa195e69" class="">Robust computing <d-cite key="Ackley2013-im"></d-cite> represents a fundamental shift in system design, prioritizing reliable operation under real-world conditions. In contrast to traditional computing, which relies on precise, error-free components, robust systems are designed to remain functional even in the face of hardware failures, environmental interference, unexpected inputs, or manufacturing variations. While contemporary computing, especially distributed computing, has some affordances around robustness to certain types of failures, it&#x27;s generally still far more brittle than any similarly complex system in the natural world, and those affordances are usually designed around very specific failure cases that we are unable to control for by other means (think cosmic ray-induced bit flips in RAM). In the example reported above, we observed how the DiffLogic CA learned rules that exhibit both fault tolerance and self-healing behavior, without explicitly designing around these conditions. When some cells fail, the damage is contained, and the system continues to function with a gradual decline rather than experiencing catastrophic failure. This mirrors how biological systems achieve reliability through networks of imperfect components, suggesting a powerful approach for future computing systems that can maintain functionality even under imperfect conditions.</p><p id="1980a320-00ae-80e1-acd5-c24e897ad167" class="">
</p><h3 id="1930a320-00ae-8016-8dd7-d421f595e190" class="">Asynchronicity </h3><p id="1960a320-00ae-8082-a939-e7211db1d6ed" class="">Inspired by the approach used in traditional NCA training <d-cite key="Niklasson2021-ft"></d-cite>, we explored asynchronous updates. Instead of updating all cells simultaneously (which can be likened to a global clock), we randomly select a subset of cells to update in each step. This simulates a scenario where each cell operates with its own internal clock. Within this framework, each cell can be conceptualized as a tiny computational unit operating independently of other cells, making its own decisions.</p><p id="19c0a320-00ae-8002-b441-cebc9f9c150c" class="">We proceeded directly to introducing asynchronicity to training, expecting this to be markedly more difficult than in traditional NCAs. Firstly, the updates at every step must output the full new state, and not just an incremental update. Secondly, a cell must now be able to account for surrounding cells being in any combination of desynchronization. Any given neighbour could be one, two, three, or more steps &quot;behind&quot; or &quot;ahead&quot;. This combinatorially increases the possible transitions rules the cell has to effectively learn to deal with. To our surprise -  successful asynchronous training was relatively easy to achieve in the simplest pattern - the checkerboard. Below, we demonstrate three different, unique, reconstructions of the pattern , all starting  from the same initial state but with distinct random seeds to determine the cell update order. Despite the asynchronous nature of these updates and a more complex resulting update rule, the cells correctly reconstruct the target pattern in 50 steps, compared to the original 20.</p><p id="1a80a320-00ae-80b0-af92-c4f4a5a645eb" class="">
</p><figure id="1980a320-00ae-80be-b66e-d6355a0280ab" class="image"><a href="images/async__.gif"><img style="width:700px" src="images/async__.gif"/></a><figcaption>Asynchronous trained patterns.</figcaption></figure><p id="1980a320-00ae-80ff-8aca-ea0fc5505fa0" class="">Furthermore, the learned circuit demonstrated generalization capabilities, exhibiting successful reconstruction on larger grids and resilience to errors -  a self-healing checkerboard..</p><div id="1980a320-00ae-805c-8eb1-e3f89fc9fe84" class="column-list"><div id="1980a320-00ae-805e-a10a-fc53a3247820" style="width:50%" class="column"><figure id="1980a320-00ae-805b-a87d-fc94afd8ee18" class="image"><a href="images/generalization.gif"><img style="width:400px" src="images/generalization.gif"/></a><figcaption>Generalization with asynchronous training</figcaption></figure><p id="1a80a320-00ae-808b-ae98-d5f58b2f6b95" class="">
</p><p id="1a80a320-00ae-80f1-9485-cb6566572c9b" class="">
</p></div><div id="1980a320-00ae-80d3-a34b-ded90815deca" style="width:50%" class="column"><figure id="1980a320-00ae-8016-a53b-d2c9818b2441" class="image"><a href="images/self_healing.gif"><img style="width:400px" src="images/self_healing.gif"/></a><figcaption>Self-healing behaviour with asynchronous training</figcaption></figure></div></div><p id="1a80a320-00ae-807e-aecd-d610a20882b2" class="">The biggest surprise came when sanity checking the original synchronously trained rule, but using asynchronous inference. It works! This is surprising and further speaks to the robustness of the circuit originally discovered. </p><p id="1a80a320-00ae-8095-900a-f90ff269de71" class="">This unexpected success with asynchronous inference led us to hypothesize that models trained directly with asynchronous updates would exhibit even greater robustness. To test this, we randomly deactivate a 10x10 pixel square within the image domain at each inference time-step, as shown in the simulations below.    </p><div id="1a80a320-00ae-8040-8158-e5448c4d5f71" class="column-list"><div id="1a80a320-00ae-8039-af7f-e9f7c980f9cb" style="width:50%" class="column"><figure id="1a80a320-00ae-80e9-8975-d78b301a6643" class="image"><a href="images/asyncronous_bomb.gif"><img style="width:400px" src="images/asyncronous_bomb.gif"/></a><figcaption>Trained Asynchronously</figcaption></figure></div><div id="1a80a320-00ae-8032-99b7-c3b6bf4bc1d1" style="width:50%" class="column"><figure id="1a80a320-00ae-8001-ae36-ebcc05db6ea2" class="image"><a href="images/syncronous_bomb.gif"><img style="width:400px" src="images/syncronous_bomb.gif"/></a><figcaption>Trained Synchronously</figcaption></figure></div></div><p id="1a80a320-00ae-80ea-a82e-f52164befdcd" class="">The images hint at the difference in resilience to noise - the asynchronous cells recover from the damage slightly more quickly, while the synchronously trained rule appears to be more impacted. By measuring the error as the sum of the absolute difference between the target and reconstructed images, we found that asynchronous training improves robustness considering these perturbations. </p><figure id="1a80a320-00ae-80c5-a841-ddf938cd000c" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mi>o</mi><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mi mathvariant="normal">∣</mi><mi>y</mi><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>t</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">Error_t = \sum_{i,j}^N |y - \tilde{y}_t|  </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">rro</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span></div></figure><figure id="1a80a320-00ae-8069-9429-cbb1d0946770" class="image"><a href="images/async_sync.svg"><img style="width:600px" src="images/async_sync.svg"/></a></figure><p id="1a80a320-00ae-80a0-b1a2-cf2604f6bb42" class="">
</p><h2 id="1900a320-00ae-801c-9a12-c65e03e9b6e5" class="">Experiment 3: Growing a Lizard</h2><p id="1900a320-00ae-80e5-b44f-e35e162e2266" class="">For the next experiment, we tested DiffLogic CA&#x27;s ability to learn arbitrary shapes by training it on the outline of a lizard, in an homage to the original NCA work. This involves more memorization than reproducing a highly-compressible regular pattern like the checkerboard. We use a cell state  of 128 bits and iterate the DiffLogic CA for 12 steps. The model architecture includes four perception circuit-kernels with 8, 4, 2, and 1 gates at each layer, respectively. The update network has 10 layers: eight layers with 512 gates each, and then layers with [256, 128] nodes, respectively. </p><h3 id="1910a320-00ae-801e-a93f-f67b742085f5" class="">Training Dataset</h3><p id="1910a320-00ae-8023-bf3a-e2874f1e8ef2" class="">We trained the model to generate a 20x20 lizard pattern, in 12  time steps. Just as in NCA, the initial condition consists of a central seed to break symmetry, with periodic boundary conditions applied to the grid edges. We employed the same loss function previously used in the checkerboard experiment.</p><figure id="1900a320-00ae-8052-8f20-c41edada6da6" class="image"><a href="images/lizard_out.png"><img style="width:250px" src="images/lizard_out.png"/></a><figcaption>Outline of a lizard</figcaption></figure><h3 id="1980a320-00ae-809f-8368-e169b4ae4207" class=""><strong>Results</strong></h3><p id="1980a320-00ae-809d-a272-d689a49e93d9" class="">To assess the model&#x27;s generalization capabilities, we evaluated its performance on a larger 40x40 grid. The results demonstrate that the model successfully learned the growth pattern without exploiting boundary conditions.</p><p id="19c0a320-00ae-8003-b682-f2ee76e35047" class="">On the left, the plot shows the convergence of both the soft and hard losses to zero. On the right, the visualization illustrates the successful growth of the lizard within the larger grid.</p><div id="1980a320-00ae-80ab-9e9b-e7ba3d378361" class="column-list"><div id="1980a320-00ae-80b0-8dfe-fa8b2b2c78c3" style="width:62.5%" class="column"><figure id="1910a320-00ae-801c-a00c-d6f1ba16ed4d" class="image"><a href="images/lizard_logo_loss.svg"><img style="width:400px" src="images/lizard_logo_loss.svg"/></a><figcaption>Loss function for the lizard. </figcaption></figure></div><div id="1980a320-00ae-80ff-9538-dc8a323d7b67" style="width:50%" class="column"><figure id="1980a320-00ae-80c1-90e6-c877f598dfef" class="image"><a href="images/lizard_video_bigger.gif"><img style="width:250px" src="images/lizard_video_bigger.gif"/></a><figcaption>Growing Lizard with diffLogic CA</figcaption></figure><p id="1980a320-00ae-80c9-91d0-da93733bc2d3" class="">
</p></div></div><p id="1980a320-00ae-80a2-88ce-e27e102d1872" class="">Below, visualizations of the first 32 hidden states offer a glimpse into the internal dynamics of the model during the growth process.</p><p id="1980a320-00ae-80fd-8f0d-f163e0f6aa46" class="">
</p><figure id="1910a320-00ae-8022-ad8e-c742a50946a9" class="image"><a href="images/lizard_32_channels.gif"><img style="width:480px" src="images/lizard_32_channels.gif"/></a><figcaption>Visualization of the first 32 channels. </figcaption></figure><p id="1980a320-00ae-8031-82aa-d59dd76aa6fa" class="">Training DiffLogic CA to generate complex patterns presents significant optimization challenges. The process required extensive hyper-parameter tuning. Future improvements to both the model architecture and circuit topology could enhance convergence speed and stability, potentially reducing the need for such intensive hyper-parameter optimization.</p><h3 id="1980a320-00ae-8000-9039-f9308ef536fc" class="">Analysis of the Generated Circuit</h3><p id="1980a320-00ae-80dc-9b30-f003157154a3" class="">A total of 577 active gates were used, excluding pass-through gates A and B.</p><p id="19a0a320-00ae-80ba-a25c-cd71756eeb32" class="">The perception kernels predominantly employed the TRUE gate, while the update circuit employed almost all available gates.</p><div id="1980a320-00ae-8004-a997-eccaa23b0192" class="column-list"><div id="1980a320-00ae-802a-8ca2-dc113c04a06a" style="width:50%" class="column"><figure id="1980a320-00ae-807c-856d-cbcf078e0706" class="image"><a href="images/lizard_perceive_gates.svg"><img style="width:400px" src="images/lizard_perceive_gates.svg"/></a><figcaption>Distribution of gate counts across all perception kernel circuits</figcaption></figure></div><div id="1980a320-00ae-80a8-9228-dda9fb3f2193" style="width:50%" class="column"><figure id="1980a320-00ae-80d3-bf36-f9bdd38e329e" class="image"><a href="images/lizard_update_gates.svg"><img style="width:400px" src="images/lizard_update_gates.svg"/></a><figcaption>Distribution of gate counts across the update circuit. </figcaption></figure></div></div><p id="1990a320-00ae-80b5-be8d-de5e2f8fdbfe" class="">
</p><h2 id="1990a320-00ae-80dc-9f06-c609a5f81ab0" class="">Experiment 4: Learning the G with colors </h2><p id="1990a320-00ae-8038-868d-e4044bacdd59" class="">Previous experiments have primarily focused on effectively monochrome images, using the last channel for visualization purposes. Wanting to investigate more complex target states,  we trained the model to generate a 16x16 &quot;colored&quot; image, over 15 steps. Using 64 channels per cell state, the model has four perception circuit-kernels, each with four three layers with 8, 4, and 2 gates, respectively. The update network architecture consists of 11 layers: 8 layers of 512 nodes each, and a final sequence of 3 layers with [256, 128, 64] nodes, respectively.</p><h3 id="1990a320-00ae-8005-95df-ce842d66c1fc" class="">Training Dataset</h3><p id="1990a320-00ae-80e1-a19d-f20f440dc67b" class="">The model was trained to generate a 16x16 colored letter of the alphabet (that might be reminiscent to some), over 15 steps. The initial state is fully zero, without periodic boundary conditions. Following the convention used in standard NCA <d-cite key="Mordvintsev2020-oh"></d-cite>, the first three channels represent RGB color values. However, in our case, these values are constrained to a binary representation of <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mi>s</mi></mrow><annotation encoding="application/x-tex">0s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span><span class="mord mathnormal">s</span></span></span></span></span><span>﻿</span></span> and <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>s</mi></mrow><annotation encoding="application/x-tex">1s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span><span class="mord mathnormal">s</span></span></span></span></span><span>﻿</span></span>, resulting in a palette of eight possible colors.</p><p id="1990a320-00ae-80ab-8271-fbdf71c7b738" class="">
</p><figure id="1990a320-00ae-804f-af51-e7b703801dde" class="image" style="text-align:center"><a href="images/google_logo_target.gif"><img style="width:250px" src="images/google_logo_target.gif"/></a><figcaption>Target Pattern</figcaption></figure><h3 id="1990a320-00ae-8038-9920-d7e58950365b" class="">Loss function</h3><p id="1990a320-00ae-8077-9efd-db1aaba4e056" class="">The loss function is defined as the sum of the squared differences between the predicted grid and the target grid at the final time-step, considering only the first three channels (0, 1, 2).</p><figure id="1990a320-00ae-8034-817d-f2eff8a918fb" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>N</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mn>0</mn><mo>:</mo><mn>3</mn></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mn>0</mn><mo>:</mo><mn>3</mn></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i,j}^N(y_{i,j,0:3} 
 - \tilde{y}_{i,j,0:3})^2  </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight">0</span><span class="mrel mtight">:</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1502em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6679em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">~</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight">0</span><span class="mrel mtight">:</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></div></figure><h3 id="1990a320-00ae-806f-8318-d8ed0c9b8796" class=""><strong>Results</strong></h3><p id="1990a320-00ae-8015-a847-e48d76781606" class="">The results demonstrate that the model successfully learns this colorful G. On the left, the loss function plots show the convergence of both the soft and hard losses. On the right, the  reconstruction the colorful G in 15 steps is shown.</p><div id="1990a320-00ae-8082-8ce5-c73c02457a73" style="align-items: flex-end;" class="column-list"><div id="1990a320-00ae-801b-ae06-f8c1b140d14b" style="width:50%" class="column"><figure id="1990a320-00ae-80a4-8329-c1179850382e" class="image"><a href="images/google_logo_loss.svg"><img style="width:400px" src="images/google_logo_loss.svg"/></a><figcaption>Loss function for the colored G </figcaption></figure><p id="1990a320-00ae-8054-a1e4-cb7e97ff3c6d" class="">
</p></div><div id="1990a320-00ae-80ae-a30a-e42733cae682" style="width:50%" class="column"><figure id="1990a320-00ae-80af-892b-e83ed518255f" class="image"><a href="images/google_logo%201.gif"><img style="width:300px" src="images/google_logo%201.gif"/></a><figcaption>"G" being generated by learned circuit</figcaption></figure><p id="1a80a320-00ae-80a7-9e61-eca29517401b" class="">
</p></div></div><h3 id="1990a320-00ae-8006-b6fe-c4898652a097" class="">Analysis of the Generated Circuit</h3><p id="1990a320-00ae-80f7-8ef1-e87febb0c8ce" class="">A total of 927 active gates were used (excluding pass-through gates A and B). Analysis of the learned logic gates revealed distinct distributions across perception and update networks. Notably, TRUE and FALSE gates were extensively employed in both networks, while the OR gate was the most prevalent in the update network. We note that this circuit was more complex than previous experiments, both in the difficulty of finding suitable hyperparameters and in size of the circuit.</p><div id="19a0a320-00ae-808a-995c-c61a58721963" class="column-list"><div id="19a0a320-00ae-8041-9622-e02fd397d819" style="width:50%" class="column"><figure id="19a0a320-00ae-800d-95ba-edc454c3dc58" class="image"><a href="images/google_logo_perceive_gates_(1).svg"><img style="width:400px" src="images/google_logo_perceive_gates_(1).svg"/></a><figcaption>Distribution of gate counts across all perception kernel circuits</figcaption></figure></div><div id="19a0a320-00ae-8092-a884-ef1aa2f8cb3e" style="width:50%" class="column"><figure id="19a0a320-00ae-801d-b74f-f0948a541f77" class="image"><a href="images/google_logo_update_gates_(1).svg"><img style="width:400px" src="images/google_logo_update_gates_(1).svg"/></a><figcaption>Distribution of gate counts across the update circuit. </figcaption></figure></div></div><p id="19a0a320-00ae-8060-b3f2-f01f093049af" class="">
</p><h3 id="17c0a320-00ae-8098-b773-d5d0c2587283" class="">Summary and Discussion</h3><p id="1a00a320-00ae-809b-b46a-cb16a2bdf0f3" class="">This work introduces DiffLogic CA, a novel NCA architecture and training regime, utilising a fully discrete cell state, updated using a learned, recurrent binary circuit. We replace the neural network components with Deep Differentiable Logic Networks, which bring the flexibility of differentiable training to discrete logic gates. The successful application of differentiable logic gates to cellular automata is demonstrated through two key results: replicating the rules of Conway&#x27;s Game of Life and generating patterns via learned discrete dynamics. These findings highlights the significant potential of integrating discrete logic within the framework of neural cellular automata and prove that differentiable logic gate networks can be effectively learned in recurrent architectures.  While the current model exhibits promising results in learning patterns, training it to generate more complex shapes and structures presents ongoing challenges. Potential directions for improvement include the exploration of hierarchical NCA architectures and the incorporation of specialized gates designed to facilitate state forgetting. For instance, integrating LSTM-like gating mechanisms into the state update process could enable a richer and diverse combination of past and newly computed candidate states, potentially enhancing the model&#x27;s dynamics and expressiveness.</p><p id="1a10a320-00ae-80c1-9b2b-d1dcb827b115" class="">
</p><h3 id="17c0a320-00ae-8098-b773-d5d0c2587283" class="">Acknowledgments</h3><p id="1a00a320-00ae-809b-b46a-cb16a2bdf0f3" class="">We thank Blaise Aguera y Arcas for his support and the Paradigm of Intelligence Team for the fruitful and inspiring discussions. Many thanks to Marek Materzok, and the contributors to the excellent <a href="https://github.com/tilk/digitaljs">DigitalJS circuit visualization library</a>, which was used to power all the interactive circuits in this article.</p><p id="1a10a320-00ae-80c1-9b2b-d1dcb827b115" class="">
</p>
</article><span class="sans" style="font-size:14px;padding-top:2em"></span>


<d-appendix>
	<style>
	d-appendix {
		contain: layout style;
	}

	d-appendix .citation {
		font-size: 11px;
		line-height: 15px;
		border-left: 1px solid rgba(0, 0, 0, 0.1);
		padding-left: 18px;
		border: 1px solid rgba(0,0,0,0.1);
		background: rgba(0, 0, 0, 0.02);
		padding: 10px 18px;
		border-radius: 3px;
		color: rgba(150, 150, 150, 1);
		overflow: hidden;
		margin-top: -12px;
		white-space: pre-wrap;
		word-wrap: break-word;
	}

	d-appendix > p {
		margin-bottom: 1em;
	}

	d-appendix > * {
		grid-column: text;
	}
	</style>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
	<h3 id="citation">Citation</h3>
    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Miotti, et al., "Differentiable Logic CA: from Game of Life to Pattern Generation", Paradigms of Intelligence, 2025.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@MISC{Pietro-Miotti-Eyvind-Niklasson-Ettore-Randazzo-Alexander-Mordvintsev2025-jb,
  title        = "Differentiable Logic {CA}: from Game of Life to Pattern
                  Generation",
  author       = "{Pietro Miotti, Eyvind Niklasson, Ettore Randazzo, Alexander
                  Mordvintsev}",
  booktitle    = "Paradigms of Intelligence",
  month        =  mar,
  year         =  2025,
  howpublished = "\url{https://google-research.github.io/self-organising-systems/difflogic-ca}",
}</pre>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>
  
  <script>
    const $$$ = q => document.documentElement.querySelectorAll(q);
    $$$('.vidoverlay').forEach(e => e.onclick = () => {
      e.parentNode.getElementsByTagName('video')[0].onended = v => {e.style.opacity = 0.8; v.target.load();};
      e.parentNode.getElementsByTagName('video')[0].currentTime = 0.0;
      e.parentNode.getElementsByTagName('video')[0].play();
      e.style.opacity = '0';
    });
  </script>

</body></html>
